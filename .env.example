# DevPlan Orchestrator Environment Variables

# LLM Provider Selection
# Options: openai, generic, requesty
LLM_PROVIDER=openai

# OpenAI API Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_ORG_ID=your-org-id-here  # Optional

# Requesty API Configuration
REQUESTY_API_KEY=your-requesty-api-key-here
REQUESTY_BASE_URL=https://api.requesty.example.com  # Optional

# Generic OpenAI-Compatible API Configuration
GENERIC_API_KEY=your-generic-api-key-here
GENERIC_BASE_URL=https://api.example.com/v1  # OpenAI-compatible endpoint
GENERIC_MODEL=gpt-4  # Model identifier

# Configuration File Path (Optional)
# Override default config/config.yaml
CONFIG_PATH=config/config.yaml

# Output and State Directories (Optional)
OUTPUT_DIR=./docs
STATE_DIR=./.devussy_state

# Logging Configuration (Optional)
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE=logs/devussy.log

# Git Configuration (Optional)
GIT_ENABLED=true  # Enable/disable automatic git commits
GIT_AUTO_PUSH=false  # Automatically push commits to remote

# Pipeline Configuration (Optional)
MAX_CONCURRENT_REQUESTS=5
STREAMING_ENABLED=false
ENABLE_CHECKPOINTS=true

# IRC Configuration
# For local development, this defaults to a direct WebSocket on localhost:8080.
# In production behind nginx, set this to the /ws/irc/ gateway path on your host,
# for example: wss://dev.ussy.host/ws/irc/
NEXT_PUBLIC_IRC_WS_URL=ws://localhost:8080
NEXT_PUBLIC_IRC_CHANNEL=#devussy-chat
