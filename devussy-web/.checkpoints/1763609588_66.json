{
  "projectName": "token muncher",
  "languages": "Python",
  "requirements": "Token Muncher is a single-player, turn-based Pac\u2011Man\u2013style roguelite played in the terminal. The player eats tokens, avoids enemies, gains XP to level up, finds loot and skills, and unlocks persistent meta-progression between runs; points earned after a run can be spent on upgrades. Maps are procedurally generated mazes designed to be simple and low-noise for navigation.",
  "design": {
    "project_name": "token muncher",
    "objectives": [
      "No objectives parsed"
    ],
    "tech_stack": [
      "Primary option: `tcod` (libtcod bindings via python-tcod)",
      "Why: Battle-tested in roguelike dev; provides console rendering, input handling, FOV, pathfinding, and good examples.",
      "Pros: Fast, feature-rich, suited for ASCII/terminal games.",
      "Cons: Adds a C binding dependency; slightly heavier than pure Python.",
      "Lightweight alternative: `rich` + `readchar`",
      "Why: `rich` provides colored text, high-quality terminal control; `readchar` simplifies single-key input.",
      "Best when you want minimal native dependencies; slightly more work for rendering grids.",
      "Cross-platform note: `curses` is an option but less ergonomic and inconsistent on Windows unless `windows-curses` is added.",
      "`dataclasses` (builtin) & `typing`: structured data, type hints for maintainability.",
      "`attrs` (optional): if you prefer richer dataclass features.",
      "`pydantic` (optional): if you need validation for complex persisted data.",
      "Option A: use `tcod.path` (if using tcod).",
      "Option B: implement small A* (recommended) \u2014 simple, easy to test, no external dependency.",
      "Small, embeddable DB: `sqlite3` (builtin)",
      "Why: reliable, no extra dependencies; good for saving meta-progression and analytics.",
      "Simpler: JSON or YAML files (use JSON for simplicity and portability).",
      "`poetry` (recommended) or `pip + virtualenv` if simpler.",
      "Why: clean dependency + packaging, reproducible installs.",
      "`pytest` for unit/integration tests.",
      "`mypy` for static typing checks.",
      "`black` for formatting, `ruff` or `flake8` for linting.",
      "GitHub Actions: run tests, linting, optionally build/release artifacts.",
      "Optionally: Releases to PyPI or GitHub Releases (packaged as a wheel).",
      "Python `logging` module, optionally `structlog` if structured logs are desired.",
      "`pre-commit` hooks to enforce formatting and checks locally.",
      "Use libraries that reduce low-level terminal handling and provide tested algorithms (tcod) where helpful. Prefer builtin or lightweight dependencies for simple persistence. Use standard Python tooling to maintain code quality and reproducibility.",
      "--"
    ],
    "architecture_overview": "Overall approach: Layered architecture with a central, testable game model (engine + systems) and thin platform-specific I/O (terminal renderer + input). Use an event-driven core (EventBus) to keep systems decoupled.\nTop-level components:\nPattern decisions:\nComponent responsibilities\nData & Control Flow (high-level)\nASCII/mermaid diagrams\nMermaid (flow):\n```mermaid\nflowchart LR\nInput -->|Command| Engine\nEngine --> Model\nEngine --> Systems\nSystems --> Model\nSystems --> EventBus\nEventBus --> Renderer\nRenderer --> Terminal\nEngine --> Persistence\n```\nSimple ASCII sequence:\nTurn flow pseudocode:\n```python\nwhile game_running:\naction = controller.get_player_action()\nengine.process(action)  # validate, resolve collisions, pickups, combat\nfor enemy in enemies:\nai_action = ai_system.decide(enemy, model)\nengine.process(ai_action)\nengine.resolve_end_of_turn()\nrenderer.render(model)\n```\nState determinism:",
    "dependencies": [
      "Why: terminal rendering, input, pathfinding, FOV utilities for grid games.",
      "Risks: C-bindings complexity; installation on some platforms may require build tools.",
      "Alternative: rich + readchar or curses (with windows-curses on Windows).",
      "Why: essential for unit/integration tests and for CI.",
      "Risks: none significant.",
      "Alternative: unittest (builtin), but pytest is more productive.",
      "Why: dependency and packaging management.",
      "Risks: learning curve for new contributors; alternative pip + venv works.",
      "Why: static typing and consistent style.",
      "Risks: developer friction if rules are too strict; configurable.",
      "Why: persistence for meta-progression.",
      "Risks: accidental corruption; ensure atomic writes and backups.",
      "Why: simple char input cross-platform.",
      "Risks: minor.",
      "Why: speed CI for larger test suites.",
      "Lock dependencies (poetry.lock).",
      "Provide easy install instructions and Docker dev container for reproducibility.",
      "Offer alternatives in README (how to run without tcod).",
      "Keep core game logic independent of the rendering library to reduce coupling.",
      "--"
    ],
    "challenges": [
      "Risks: curses/terminal features behave differently; tcod dependency may be tricky on Windows.",
      "Provide alternative renderer (text-only using print/ANSI) and test on Windows.",
      "Document dependencies and Windows install steps (e.g., `pip install windows-curses`).",
      "Use CI matrix to test on Linux + Windows runners.",
      "Optimize renderer drawing: only redraw changed tiles (dirty rectangles) instead of full screen.",
      "Keep map sizes reasonable by design defaults (e.g., 80x24 to 120x40).",
      "Profile critical loops; use list comprehensions and avoid excessive object allocations.",
      "Use tcod which uses efficient console blitting.",
      "Risks: generated maps that are confusing or unfair.",
      "Use robust maze algorithms (recursive backtracker or Prim\u2019s), then apply smoothing to ensure clear corridors and token placement.",
      "Generate multiple candidate maps and evaluate metrics (path lengths, connectivity, token density) to select the best.",
      "Provide seed-based reproducibility for testing.",
      "Risks: enemies that trap player unfairly or are predictable/boring.",
      "Implement simple finite-state machines per enemy (chase, scatter, patrol, frightened).",
      "Use pathfinding with distance-based heuristics, and allow randomness in behavior.",
      "Add difficulty scaling, and test enemy interactions in unit tests.",
      "Risks: progression feels meaningless or breaks game balance.",
      "Start with simple numerical systems and instrument metrics (average run length, tokens collected).",
      "Implement tweakable JSON/YAML config tables for loot/XP to iterate without code changes.",
      "Introduce analytics logs (local only) for later analysis.",
      "Risks: save corruption or incompatible versions.",
      "Use schema versioning in saved data.",
      "Write saves atomically (write-temp + rename).",
      "Provide a migration path for saved meta data.",
      "Risks: renderer/input make integration tests brittle.",
      "Provide headless renderer and mock input for tests.",
      "Keep logic in systems that can be exercised without the renderer.",
      "Risks: adding features that overcomplicate the first release.",
      "Define a Minimum Viable Product (MVP) and prioritize core loop; postpone fancy features (achievements, multiple characters) to post-MVP.",
      "Use a backlog and small iterations.",
      "Local application; minimal external attack surface.",
      "If saving remote stats in future, ensure secure transport and validate inputs.",
      "Sanitize any file paths; avoid executing untrusted code.",
      "--"
    ],
    "mitigations": [
      "Mitigations:",
      "Risks: large maps + many entities may slow down rendering and turn resolution.",
      "Mitigations:",
      "Mitigations:",
      "Mitigations:",
      "Mitigations:",
      "Mitigations:",
      "Mitigations:",
      "Mitigations:"
    ],
    "raw_llm_response": "# Token Muncher \u2014 Project Design Document\n\nVersion: 1.0  \nTarget implementation: Python, terminal-based roguelite\n\n---\n\nTable of contents\n1. High-Level Project Objectives  \n2. Technology Stack Recommendations  \n3. Architecture Overview  \n4. Key Dependencies  \n5. Project Structure  \n6. Potential Challenges and Mitigations  \n7. Development Approach\n\n---\n\n## 1. High-Level Project Objectives\n\n### Purpose and primary goal\n- Build a single-player, turn-based, terminal-playable roguelite inspired by Pac\u2011Man:\n  - Player navigates procedurally generated mazes, eats tokens, avoids enemies, gains XP, levels up, finds loot and skills.\n  - Runs contribute persistent meta-progression points that can be spent on permanent upgrades between runs.\n- Deliver a polished, fun, replayable game playable entirely in a terminal.\n\n### Problems it solves / value proposition\n- Provides a lightweight, accessible roguelite experience that runs in a terminal (low system requirements).\n- Demonstrates good procedural generation, enemy AI, and a simple persistent progression system in an approachable codebase.\n- Offers a small, well-structured project for learning game programming, algorithms (mazes, A*), and architecture in Python.\n\n### Target user / audience\n- Terminal users who enjoy roguelikes/roguelites and quick runs.\n- Developers learning game mechanics and procedural generation.\n- Players who prefer keyboard-only, text/ASCII art aesthetics.\n\n### Key success criteria (measurable)\n- Core loop implemented: map generation, token collection, enemy behavior, XP/leveling, loot/skills, run termination.\n- Playability: run from start to finish without crashes; maps fit typical terminal sizes (>= 80x24).\n- Persistence: points earned persist between runs and can be spent in a meta menu.\n- Performance: turn response < 100ms on typical development machine for maps up to 120x40.\n- Test coverage: critical game logic (map gen, pathfinding, combat, persistence) covered by unit tests.\n- Developer ergonomics: developer can start running the game and tests with 2 commands (e.g., install + run or just `poetry install && poetry run token-muncher`).\n\n---\n\n## 2. Technology Stack Recommendations\n\nPrimary language: Python (as specified)\n\nRecommended libraries/tools (with justification):\n\n1. Terminal Rendering & Input\n   - Primary option: `tcod` (libtcod bindings via python-tcod)\n     - Why: Battle-tested in roguelike dev; provides console rendering, input handling, FOV, pathfinding, and good examples.\n     - Pros: Fast, feature-rich, suited for ASCII/terminal games.\n     - Cons: Adds a C binding dependency; slightly heavier than pure Python.\n   - Lightweight alternative: `rich` + `readchar`\n     - Why: `rich` provides colored text, high-quality terminal control; `readchar` simplifies single-key input.\n     - Best when you want minimal native dependencies; slightly more work for rendering grids.\n   - Cross-platform note: `curses` is an option but less ergonomic and inconsistent on Windows unless `windows-curses` is added.\n\n2. Game architecture & utilities\n   - `dataclasses` (builtin) & `typing`: structured data, type hints for maintainability.\n   - `attrs` (optional): if you prefer richer dataclass features.\n   - `pydantic` (optional): if you need validation for complex persisted data.\n\n3. Pathfinding and grid algorithms\n   - Option A: use `tcod.path` (if using tcod).\n   - Option B: implement small A* (recommended) \u2014 simple, easy to test, no external dependency.\n\n4. Persistence and configuration\n   - Small, embeddable DB: `sqlite3` (builtin)\n     - Why: reliable, no extra dependencies; good for saving meta-progression and analytics.\n   - Simpler: JSON or YAML files (use JSON for simplicity and portability).\n\n5. Packaging & dependency management\n   - `poetry` (recommended) or `pip + virtualenv` if simpler.\n     - Why: clean dependency + packaging, reproducible installs.\n\n6. Testing & quality\n   - `pytest` for unit/integration tests.\n   - `mypy` for static typing checks.\n   - `black` for formatting, `ruff` or `flake8` for linting.\n\n7. CI/CD\n   - GitHub Actions: run tests, linting, optionally build/release artifacts.\n   - Optionally: Releases to PyPI or GitHub Releases (packaged as a wheel).\n\n8. Logging\n   - Python `logging` module, optionally `structlog` if structured logs are desired.\n\n9. Dev tools\n   - `pre-commit` hooks to enforce formatting and checks locally.\n\nJustification summary:\n- Use libraries that reduce low-level terminal handling and provide tested algorithms (tcod) where helpful. Prefer builtin or lightweight dependencies for simple persistence. Use standard Python tooling to maintain code quality and reproducibility.\n\n---\n\n## 3. Architecture Overview\n\nOverall approach: Layered architecture with a central, testable game model (engine + systems) and thin platform-specific I/O (terminal renderer + input). Use an event-driven core (EventBus) to keep systems decoupled.\n\nTop-level components:\n- Game Engine / Loop\n- Game Model (world state)\n- Entities & Component System (lightweight)\n- Systems (Movement, AI, Combat, Loot, XP/Leveling, Spawn, Persistence)\n- Renderer (terminal view)\n- Input Controller\n- Persistence Manager (saves meta-progression)\n- EventBus / Message System\n- Configuration & Resources (game constants, tiers, loot tables)\n\nPattern decisions:\n- MVC-ish: Model = game state & systems; View = Renderer; Controller = Input handler. Use EventBus to mediate events (turns, death, item pickup).\n- ECS-like but minimal: Entities are dataclasses with components expressed as attributes rather than full-blown ECS library \u2014 aim for simplicity and testability.\n\nComponent responsibilities\n- Game Engine\n  - Maintain main loop (turn-based).\n  - Coordinate player turn vs enemy turns.\n  - Emit events (TurnStart, ActionPerformed, EntityDied).\n- Game Model\n  - Maze map + tile types, items, tokens.\n  - Entity list with positions and attributes (HP, speed, inventory, XP).\n  - Random seed / RNG state for reproducibility.\n- Systems\n  - MovementSystem: validation, collision checks, token pickups.\n  - AISystem: enemy behavior (chase, scatter, flee, random walk).\n  - CombatSystem: resolve damage and deaths.\n  - LootSystem: generate and apply loot/skills.\n  - XPSystem: reward XP, handle level-ups.\n  - MetaSystem: track earned points and apply persistent upgrades between runs.\n- Renderer\n  - Draw map and entities to terminal; show HUD (HP, XP, level, tokens, run points).\n  - Support different rendering backends (adapter pattern) so unit tests can mock rendering.\n- Input Controller\n  - Read keystrokes -> translate to commands (move, wait, use skill, open shop, exit).\n- Persistence Manager\n  - Read/write persistent meta state (points, unlocks).\n  - Save run logs (optional).\n- EventBus\n  - Publish/subscribe simple event objects so systems can react without tight coupling.\n\nData & Control Flow (high-level)\n- Player issues action via Input -> Controller transforms to Action -> Engine validates & sends to relevant Systems -> Systems update Model -> Systems publish Events (pickup, kill) -> Renderer subscribes to Events to animate/flash -> Engine advances enemy turns -> Repeat until run end -> Persistence Manager stores points.\n\nASCII/mermaid diagrams\n\nMermaid (flow):\n```mermaid\nflowchart LR\n  Input -->|Command| Engine\n  Engine --> Model\n  Engine --> Systems\n  Systems --> Model\n  Systems --> EventBus\n  EventBus --> Renderer\n  Renderer --> Terminal\n  Engine --> Persistence\n```\n\nSimple ASCII sequence:\n- [User Input] -> [Controller] -> [Engine] -> [MovementSystem / AISystem / CombatSystem] -> [Model updated] -> [EventBus] -> [Renderer] -> [Terminal]\n\nTurn flow pseudocode:\n```python\nwhile game_running:\n    action = controller.get_player_action()\n    engine.process(action)  # validate, resolve collisions, pickups, combat\n    for enemy in enemies:\n        ai_action = ai_system.decide(enemy, model)\n        engine.process(ai_action)\n    engine.resolve_end_of_turn()\n    renderer.render(model)\n```\n\nState determinism:\n- Engine carries RNG seed per run; seed saved for replayability.\n\n---\n\n## 4. Key Dependencies\n\nRecommended critical dependencies and alternatives:\n\n1. python-tcod (tcod)\n   - Why: terminal rendering, input, pathfinding, FOV utilities for grid games.\n   - Risks: C-bindings complexity; installation on some platforms may require build tools.\n   - Alternative: rich + readchar or curses (with windows-curses on Windows).\n\n2. pytest\n   - Why: essential for unit/integration tests and for CI.\n   - Risks: none significant.\n   - Alternative: unittest (builtin), but pytest is more productive.\n\n3. poetry\n   - Why: dependency and packaging management.\n   - Risks: learning curve for new contributors; alternative pip + venv works.\n\n4. mypy, black, ruff\n   - Why: static typing and consistent style.\n   - Risks: developer friction if rules are too strict; configurable.\n\n5. sqlite3 (builtin) or JSON files\n   - Why: persistence for meta-progression.\n   - Risks: accidental corruption; ensure atomic writes and backups.\n\n6. readchar (if not using tcod)\n   - Why: simple char input cross-platform.\n   - Risks: minor.\n\n7. Optional: pytest-xdist for parallel testing\n   - Why: speed CI for larger test suites.\n\nDependency risk mitigation:\n- Lock dependencies (poetry.lock).\n- Provide easy install instructions and Docker dev container for reproducibility.\n- Offer alternatives in README (how to run without tcod).\n- Keep core game logic independent of the rendering library to reduce coupling.\n\n---\n\n## 5. Project Structure\n\nRecommended directory layout (top-level):\n\n```\ntoken_muncher/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 CHANGELOG.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/ci.yml\n\u251c\u2500\u2500 docs/\n\u2502   \u2514\u2500\u2500 design.md\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 token_muncher/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 main.py\n\u2502       \u251c\u2500\u2500 config.py\n\u2502       \u251c\u2500\u2500 engine.py\n\u2502       \u251c\u2500\u2500 eventbus.py\n\u2502       \u251c\u2500\u2500 model/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u251c\u2500\u2500 map.py\n\u2502       \u2502   \u251c\u2500\u2500 entity.py\n\u2502       \u2502   \u2514\u2500\u2500 components.py\n\u2502       \u251c\u2500\u2500 systems/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u251c\u2500\u2500 movement.py\n\u2502       \u2502   \u251c\u2500\u2500 ai.py\n\u2502       \u2502   \u251c\u2500\u2500 combat.py\n\u2502       \u2502   \u251c\u2500\u2500 loot.py\n\u2502       \u2502   \u2514\u2500\u2500 xp.py\n\u2502       \u251c\u2500\u2500 renderer/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u251c\u2500\u2500 base_renderer.py\n\u2502       \u2502   \u251c\u2500\u2500 tcod_renderer.py\n\u2502       \u2502   \u2514\u2500\u2500 text_renderer.py\n\u2502       \u251c\u2500\u2500 input/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 keyboard.py\n\u2502       \u251c\u2500\u2500 persistence/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 meta_store.py\n\u2502       \u2514\u2500\u2500 utils/\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u2514\u2500\u2500 a_star.py\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 unit/\n\u2502   \u2514\u2500\u2500 integration/\n\u2514\u2500\u2500 scripts/\n    \u2514\u2500\u2500 run_local.sh\n```\n\nExplanation and organization:\n- src/token_muncher/: main package. Keeps code importable in editable installs.\n- model/: map, entity definitions, and pure state. No I/O here.\n- systems/: pure game logic that operates on model. Testable and independent of renderer/i/o.\n- renderer/: abstract base and specific implementations. The rest of the code depends on the base interface.\n- input/: keyboard input handling; separate adapter for test mocks.\n- persistence/: saving/loading meta-progression and runtime settings.\n- utils/: algorithms like A*, RNG helpers.\n- tests/: unit (model and systems), integration (engine + renderer in headless mode).\n- docs/: game design docs, loot tables, balancing notes.\n- scripts/: helper scripts for dev, packaging.\n\nCode organization best practices:\n- Keep side-effecting code out of module import time. main.py should be entrypoint.\n- Keep pure logic decoupled from rendering; pass state objects or use an interface.\n- Favor small, single-responsibility modules.\n- Use type hints and docstrings.\n\nExample minimal entrypoint (src/token_muncher/main.py):\n```python\ndef main():\n    config = load_config()\n    persistence = MetaStore(config.meta_path)\n    renderer = get_renderer(config)\n    controller = KeyboardController()\n    engine = Engine(config, persistence)\n    engine.run(renderer, controller)\n```\n\nTesting conventions:\n- Unit tests for map generation, pathfinding, item pickup, XP logic, persistence serialization.\n- Integration tests for a few deterministic seeds that run 10-50 turns and assert invariants.\n\n---\n\n## 6. Potential Challenges and Mitigations\n\n1. Terminal compatibility (Windows vs Unix)\n   - Risks: curses/terminal features behave differently; tcod dependency may be tricky on Windows.\n   - Mitigations:\n     - Provide alternative renderer (text-only using print/ANSI) and test on Windows.\n     - Document dependencies and Windows install steps (e.g., `pip install windows-curses`).\n     - Use CI matrix to test on Linux + Windows runners.\n\n2. Rendering/performance bottlenecks\n   - Risks: large maps + many entities may slow down rendering and turn resolution.\n   - Mitigations:\n     - Optimize renderer drawing: only redraw changed tiles (dirty rectangles) instead of full screen.\n     - Keep map sizes reasonable by design defaults (e.g., 80x24 to 120x40).\n     - Profile critical loops; use list comprehensions and avoid excessive object allocations.\n     - Use tcod which uses efficient console blitting.\n\n3. Procedural generation quality (too noisy / hard to navigate)\n   - Risks: generated maps that are confusing or unfair.\n   - Mitigations:\n     - Use robust maze algorithms (recursive backtracker or Prim\u2019s), then apply smoothing to ensure clear corridors and token placement.\n     - Generate multiple candidate maps and evaluate metrics (path lengths, connectivity, token density) to select the best.\n     - Provide seed-based reproducibility for testing.\n\n4. Enemy AI complexity / fairness\n   - Risks: enemies that trap player unfairly or are predictable/boring.\n   - Mitigations:\n     - Implement simple finite-state machines per enemy (chase, scatter, patrol, frightened).\n     - Use pathfinding with distance-based heuristics, and allow randomness in behavior.\n     - Add difficulty scaling, and test enemy interactions in unit tests.\n\n5. Balancing XP, loot, meta progression\n   - Risks: progression feels meaningless or breaks game balance.\n   - Mitigations:\n     - Start with simple numerical systems and instrument metrics (average run length, tokens collected).\n     - Implement tweakable JSON/YAML config tables for loot/XP to iterate without code changes.\n     - Introduce analytics logs (local only) for later analysis.\n\n6. Persistence integrity and upgrades\n   - Risks: save corruption or incompatible versions.\n   - Mitigations:\n     - Use schema versioning in saved data.\n     - Write saves atomically (write-temp + rename).\n     - Provide a migration path for saved meta data.\n\n7. Testability with terminal I/O\n   - Risks: renderer/input make integration tests brittle.\n   - Mitigations:\n     - Provide headless renderer and mock input for tests.\n     - Keep logic in systems that can be exercised without the renderer.\n\n8. Scope creep\n   - Risks: adding features that overcomplicate the first release.\n   - Mitigations:\n     - Define a Minimum Viable Product (MVP) and prioritize core loop; postpone fancy features (achievements, multiple characters) to post-MVP.\n     - Use a backlog and small iterations.\n\nSecurity considerations:\n- Local application; minimal external attack surface.\n- If saving remote stats in future, ensure secure transport and validate inputs.\n- Sanitize any file paths; avoid executing untrusted code.\n\n---\n\n## 7. Development Approach\n\nRecommended methodology\n- Agile / Iterative with short sprints (1\u20132 weeks).\n- Start with an MVP scope focused on a playable single-run loop and basic persistence.\n- Keep issues small & actionable; use a kanban board or GitHub Projects.\n\nSuggested milestones\n1. MVP (2\u20134 weeks)\n   - Minimal dev environment, CI, and packaging.\n   - Map generation, player movement, token pickups, basic renderer, input handling, engine loop.\n2. Enemies & AI (1\u20132 weeks)\n   - Enemy entities, basic chase behavior, collisions and death.\n3. XP/Leveling & Loot (1\u20132 weeks)\n   - XP rewards, simple skills, loot drops.\n4. Persistence & Meta (1 week)\n   - Points earned and persistent upgrades between runs.\n5. Polish & Balancing (1\u20132 weeks)\n   - HUD, sound/notifications (beeps), config tuning, test coverage.\n6. Release prep\n   - Packaging, docs, and user guide.\n\nTesting strategy\n- Unit tests:\n  - Map generation properties (connectivity), A* pathfinding correctness, systems (XP, loot), persistence serialization.\n- Integration tests:\n  - Headless run of engine with deterministic seed for a fixed number of turns to ensure no exceptions and invariant properties.\n- End-to-end (manual):\n  - Playtesting sessions on major platforms (Linux, macOS, Windows) and record issues.\n- Test tooling:\n  - pytest with fixtures for deterministic RNG seeds.\n  - Use mocks for renderer and input.\n\nTDD / style:\n- Recommend TDD for critical logic (map gen, pathfinding, combat).\n- Use pre-commit hooks with black and ruff to keep codebase consistent.\n\nCI/CD\n- GitHub Actions workflow:\n  - On push / PR: install dependencies, run linters, mypy, run pytest.\n  - On tag/release: build wheel (poetry build) and optionally publish to PyPI or upload release artifact.\n- Release cadence:\n  - Use semantic versioning (MAJOR.MINOR.PATCH).\n  - Keep changelog updated with notable changes.\n\nDocumentation\n- README with quick start, controls, and dev setup.\n- docs/design.md with game rules, number values, and procedural generation explanation.\n- Inline docstrings and type hints for maintainability.\n- CONTRIBUTORS / CONTRIBUTING guide with style and how to run tests locally.\n- Add a simple playtest checklist and known issues file.\n\nExample developer commands\n- Install & run:\n  - With poetry: `poetry install && poetry run token-muncher`\n- Run tests:\n  - `poetry run pytest`\n- Lint:\n  - `poetry run ruff check src tests && poetry run black --check .`\n\n---\n\n## Appendices\n\n### Example entity dataclass\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Entity:\n    id: int\n    name: str\n    x: int\n    y: int\n    hp: int\n    max_hp: int\n    speed: int\n    is_player: bool = False\n    inventory: list = None\n```\n\n### Minimal A* API (utils/a_star.py)\n- `find_path(grid, start, goal, passable_fn)` -> list[(x,y)] or None\n- Include unit tests for diagonal vs orthogonal moves and obstacles.\n\n### Suggested default game config (config.py)\n- Map size: width=80, height=24\n- Token count per map: function of area (e.g., area * 0.04)\n- Enemy count: 2\u20135 depending on difficulty\n- XP per token: 1\n- XP to level: level^2 * base_xp\n\n---\n\nIf you\u2019d like, I can:\n- Produce a prioritized backlog (user stories + estimates).\n- Provide concrete MVP tasks with time estimates for a solo developer.\n- Draft sample code for the engine loop, map generator, or A* pathfinding."
  },
  "plan": {
    "phases": [
      {
        "number": 1,
        "title": "Project Initialization & Tooling",
        "description": null,
        "steps": [
          {
            "number": "1.1",
            "description": "Create repository root and base files",
            "details": [
              "Run: mkdir token-muncher && cd token-muncher",
              "Create README: echo \"# token muncher\" > README.md",
              "Create .gitignore with Python and editor ignores: create file .gitignore containing:",
              "__pycache__/, *.py[cod], .venv/, .env, .DS_Store, .pytest_cache/, .mypy_cache/",
              "Create LICENSE (MIT): create file LICENSE with standard MIT license text (replace owner/year as needed)",
              "Create a docs/ directory for later documentation: mkdir docs",
              "Acceptance checks:",
              "Files README.md, LICENSE, .gitignore, and docs/ exist at repository root",
              "Running ls shows those files and directory"
            ],
            "done": false
          },
          {
            "number": "1.2",
            "description": "Commit initial repository files",
            "details": [
              "Run: git init",
              "Run: git add README.md LICENSE .gitignore docs/",
              "Run: git commit -m \"chore: repository skeleton and base files\"",
              "Acceptance checks:",
              "git log -1 shows the commit message above",
              "git status shows a clean working tree"
            ],
            "done": false
          },
          {
            "number": "1.3",
            "description": "Initialize Python project metadata with Poetry (dependency manager)",
            "details": [
              "Install Poetry if not installed (system step): curl -sSL https://install.python-poetry.org | python3 -",
              "Run: poetry init --name token-muncher --description \"CLI tool to munch tokens\" --author \"Your Name <you@example.com>\" --license MIT --dependency \"python>=3.9\" --no-interaction",
              "Run: poetry add --dev pytest black flake8 isort pre-commit",
              "Confirm pyproject.toml exists and contains [tool.poetry] with name \"token-muncher\" and a [tool.poetry.dependencies] python spec",
              "Acceptance checks:",
              "File pyproject.toml exists at project root",
              "Command poetry show --no-dev lists project and poetry show --dev lists dev tools installed"
            ],
            "done": false
          },
          {
            "number": "1.4",
            "description": "Create package layout under src/ and entrypoint files",
            "details": [
              "Create directories and files:",
              "mkdir -p src/token_muncher",
              "touch src/token_muncher/__init__.py",
              "touch src/token_muncher/__main__.py",
              "touch src/token_muncher/cli.py",
              "touch src/token_muncher/core.py",
              "Populate src/token_muncher/__init__.py with __version__ = \"0.0.0\"",
              "Populate src/token_muncher/__main__.py with:",
              "from token_muncher.cli import main",
              "if __name__ == \"__main__\": main()",
              "Acceptance checks:",
              "Files exist at the listed paths",
              "Running python -m token_muncher prints nothing (no syntax errors)"
            ],
            "done": false
          },
          {
            "number": "1.5",
            "description": "Implement CLI entrypoint in src/token_muncher/cli.py",
            "details": [
              "Add an argparse-based CLI with:",
              "A main() function that accepts arguments: --version, --input/-i (text), --count/-c (flag)",
              "Usage and help text that explain: \"token-muncher: munch tokens from input text; --count returns token counts\"",
              "Graceful error handling: if no input provided, print user-friendly error \"No input provided. Use -i/--input or pipe text.\"",
              "Concrete contents to implement:",
              "import argparse; from token_muncher.core import TokenMuncher",
              "parse args and call TokenMuncher().munch() or .count() accordingly",
              "print JSON output for results when successful",
              "Add unit-testable behaviors:",
              "main should return exit code 0 on success, non-zero on failure",
              "Acceptance checks:",
              "Run: python -m token_muncher --help and verify the help includes the flags and descriptive help text",
              "Run: python -m token_muncher --version prints \"token-muncher 0.0.0\"",
              "Run: echo \"hello world\" | python -m token_muncher -i \"hello world\" and no stack trace appears (may print result)"
            ],
            "done": false
          },
          {
            "number": "1.6",
            "description": "Implement core TokenMuncher in src/token_muncher/core.py",
            "details": [
              "Create TokenMuncher class with:",
              "class TokenMuncher:",
              "def tokenize(self, text: str) -> list[str]: splits on whitespace and basic punctuation; lowercases tokens; returns list",
              "def munch(self, text: str) -> dict: returns {\"tokens\": [...], \"count\": n}",
              "def count(self, text: str) -> dict: returns frequency mapping {\"token\": count}",
              "Include type hints and short docstrings for each method",
              "Add simple error handling: raise ValueError(\"text must be a non-empty string\") for invalid input",
              "Add small helper functions if useful, e.g., _normalize_token(token: str) -> str",
              "Acceptance checks:",
              "From project root run: poetry run python -c \"from token_muncher.core import TokenMuncher; print(TokenMuncher().munch('Hello, world!'))\" and verify output contains tokens [\"hello\",\"world\"]",
              "Running TokenMuncher().count(\"a a b\") returns {\"a\":2,\"b\":1}"
            ],
            "done": false
          },
          {
            "number": "1.7",
            "description": "Add console script entrypoint to pyproject.toml so package is runnable as token-muncher",
            "details": [
              "Edit pyproject.toml to include:",
              "[tool.poetry.scripts]",
              "Run: poetry install to register the script in the virtualenv",
              "Test the console script:",
              "Run: poetry run token-muncher --help and verify help output appears",
              "Acceptance checks:",
              "poetry run token-muncher --version prints version",
              "poetry run token-muncher -i \"one two\" prints expected tokens or count without error"
            ],
            "done": false
          },
          {
            "number": "1.8",
            "description": "Write unit tests for core and CLI in tests/",
            "details": [
              "Create tests directory and files:",
              "mkdir -p tests",
              "Create tests/test_core.py with tests:",
              "test_tokenize_basic: TokenMuncher().tokenize(\"Hello, world!\") == [\"hello\",\"world\"]",
              "test_munch_count: TokenMuncher().munch(\"a a b\")[\"count\"] == 3",
              "test_count_freq: TokenMuncher().count(\"a a b\") == {\"a\":2,\"b\":1}",
              "Create tests/test_cli.py with tests:",
              "test_cli_help: run python -m token_muncher --help and assert exit code 0 and help text contains \"token-muncher\"",
              "test_cli_input: run token-muncher -i \"x y\" via subprocess and assert expected JSON output contains tokens",
              "Use pytest features and assert statements; do not rely on network or external state",
              "Acceptance checks:",
              "Run: poetry run pytest -q and verify tests run (initially they should pass if implementations are correct)",
              "Tests should be deterministic and not flaky"
            ],
            "done": false
          },
          {
            "number": "1.9",
            "description": "Commit feature implementation and tests",
            "details": [
              "Run: git add pyproject.toml src/token_muncher/ tests/",
              "Run: git commit -m \"feat: add core TokenMuncher implementation and CLI with tests\"",
              "Acceptance checks:",
              "git log -1 includes the above message",
              "All new files are staged and committed (git show --name-only HEAD)"
            ],
            "done": false
          },
          {
            "number": "1.10",
            "description": "Add linting/formatting configuration and pre-commit",
            "details": [
              "Add configuration files:",
              "Update pyproject.toml to configure black and isort (e.g., [tool.black] line-length = 88)",
              "Create .flake8 with content:",
              "[flake8]",
              "Create .pre-commit-config.yaml including hooks:",
              "repo: https://github.com/psf/black, rev: stable, hooks: - id: black",
              "repo: https://github.com/PyCQA/isort, rev: 5.12.0, hooks: - id: isort",
              "repo: https://github.com/pre-commit/mirrors-flake8, rev: 5.0.4, hooks: - id: flake8",
              "Install pre-commit hooks into local repo:",
              "Run: poetry run pre-commit install",
              "Acceptance checks:",
              ".pre-commit-config.yaml and .flake8 exist at repo root",
              "pre-commit install run reports \"pre-commit installed\""
            ],
            "done": false
          },
          {
            "number": "1.11",
            "description": "Run formatters and linters, fix issues",
            "details": [
              "Format code:",
              "Run: poetry run black src/ tests/",
              "Run: poetry run isort src/ tests/",
              "Lint code:",
              "Run: poetry run flake8 src/ tests/",
              "Fix any warnings/errors flagged by flake8 in code (variable names, unused imports)",
              "Run pre-commit locally as a full check:",
              "Run: poetry run pre-commit run --all-files",
              "Acceptance checks:",
              "black and isort complete without modifying files (or after they modify, run git add and commit)",
              "flake8 exits with code 0",
              "pre-commit run --all-files exits 0"
            ],
            "done": false
          },
          {
            "number": "1.12",
            "description": "Commit formatting and tooling configuration",
            "details": [
              "Run: git add .pre-commit-config.yaml .flake8 pyproject.toml",
              "Run: git add src/ tests/ (if black/isort changed files)",
              "Run: git commit -m \"chore: add formatting and linting config; apply black/isort\"",
              "Acceptance checks:",
              "git log -1 shows the commit message",
              "pre-commit hooks are recorded and run on future commits"
            ],
            "done": false
          },
          {
            "number": "1.13",
            "description": "Add CI workflow to run tests and linters on push (GitHub Actions)",
            "details": [
              "Create file .github/workflows/ci.yml with contents:",
              "name: CI",
              "on: [push, pull_request]",
              "jobs:",
              "uses: actions/checkout@v4",
              "name: Set up Python",
              "name: Install dependencies",
              "name: Lint",
              "name: Format check",
              "name: Run tests",
              "Acceptance checks:",
              "File exists at .github/workflows/ci.yml",
              "The workflow yml is valid YAML (can be tested locally with yamllint or by pushing a branch to GitHub to see GitHub Actions trigger)"
            ],
            "done": false
          },
          {
            "number": "1.14",
            "description": "Commit CI workflow",
            "details": [
              "Run: git add .github/workflows/ci.yml",
              "Run: git commit -m \"chore(ci): add GitHub Actions workflow for tests and linters\"",
              "Acceptance checks:",
              "git log -1 shows the commit",
              "On push to remote, GitHub Actions will run the new workflow (verify on GitHub)"
            ],
            "done": false
          },
          {
            "number": "1.15",
            "description": "Add user-facing documentation and examples in README.md",
            "details": [
              "Update README.md to include:",
              "Project description: \"token muncher \u2014 simple CLI for tokenizing and counting tokens\"",
              "Installation instructions using Poetry: poetry install && poetry run token-muncher -i \"text\"",
              "Quick usage examples:",
              "token-muncher -i \"Hello world\"",
              "token-muncher -i \"a a b\" --count",
              "Contributing section with how to run tests: poetry run pytest",
              "Add changelog stub: create CHANGELOG.md with \"## Unreleased\"",
              "Acceptance checks:",
              "README.md contains example commands and expected output snippets",
              "Running one of the README examples reproduces the documented behavior"
            ],
            "done": false
          },
          {
            "number": "1.16",
            "description": "Finalize initial phase with a commit for docs and meta",
            "details": [
              "Run: git add README.md CHANGELOG.md LICENSE",
              "Run: git commit -m \"docs: add README usage examples and CHANGELOG stub\"",
              "Acceptance checks:",
              "git log shows the final docs commit",
              "Repository contains working CLI, tests, linting, pre-commit, and CI configuration enabling Phase 1 completion"
            ],
            "done": false
          }
        ]
      },
      {
        "number": 2,
        "title": "Core Game Model & Engine",
        "description": null,
        "steps": [
          {
            "number": "2.1",
            "description": "Create project directories and baseline files",
            "details": [
              "Create directories: run `mkdir -p src/core tests docs` from the repository root.",
              "Create Python package markers: create empty files `src/__init__.py` and `src/core/__init__.py`.",
              "Create placeholder README and license: create `README.md` with one-line project description \"token muncher \u2014 core game model & engine\", and `LICENSE` (MIT or your choice).",
              "Create a minimal test runner file: create `tests/__init__.py` (empty).",
              "Acceptance checks:",
              "Files exist: `ls src core tests docs README.md LICENSE` returns created items.",
              "Python packages importable: `python -c \"import src; import src.core; print('ok')\"` prints \"ok\" or no error.",
              "Commit: `git add src/ tests/ README.md LICENSE && git commit -m \"chore: create project structure for core game model & engine\"`"
            ],
            "done": false
          },
          {
            "number": "2.2",
            "description": "Add tooling and configuration (black, flake8, pytest)",
            "details": [
              "Create `pyproject.toml` at repo root with sections for [tool.black], [tool.flake8], and [tool.pytest.ini_options]; set line-length 88 and include `src` as package dir.",
              "Create `requirements-dev.txt` with: `pytest`, `black`, `flake8`.",
              "Create `setup.cfg` or add `[flake8]` in `pyproject.toml` if preferred. Ensure flake8 ignores common patterns like `__pycache__`.",
              "Acceptance checks:",
              "Run `python -m pip install -r requirements-dev.txt` (or skip if using system).",
              "Run `black --check src/` should run (may fail until files present).",
              "Commit: `git add pyproject.toml requirements-dev.txt && git commit -m \"chore: add dev tooling config (black, flake8, pytest)\"`"
            ],
            "done": false
          },
          {
            "number": "2.3",
            "description": "Implement foundational data types: `src/core/vec.py`",
            "details": [
              "Create file `src/core/vec.py`.",
              "Implement a small immutable 2D vector dataclass:",
              "class name `Vec2` with float x, y.",
              "methods: `__add__`, `__sub__`, `__mul__` (scalar), `length()`, `distance_to(other)`, `to_tuple()`.",
              "include docstrings and simple type hints.",
              "Add unit test placeholder `tests/test_vec.py` that verifies arithmetic and distance.",
              "Acceptance checks:",
              "Run `python -c \"from src.core.vec import Vec2; print(Vec2(1,2)+Vec2(3,4))\"` prints a Vec2 representation without error.",
              "Run `pytest tests/test_vec.py` and tests pass (implement tests accordingly).",
              "Commit: `git add src/core/vec.py tests/test_vec.py && git commit -m \"feat: add Vec2 utility for positions and movement\"`"
            ],
            "done": false
          },
          {
            "number": "2.4",
            "description": "Implement core entity model: `src/core/models.py`",
            "details": [
              "Create file `src/core/models.py`.",
              "Implement dataclasses:",
              "`Entity` base class: id (str), pos (Vec2), radius (float), velocity (Vec2), alive (bool), tag (str or enum).",
              "`Player(Entity)`: additional fields score (int), lives (int), methods move(delta: Vec2), eat(token), serialize().",
              "`Token(Entity)`: fields value (int), spawn_time (float), expire_time (optional float).",
              "`Obstacle(Entity)`: immobile entity, may block movement.",
              "Add explicit validation in constructors (radius > 0).",
              "Add serialization helpers: `to_dict()` and `from_dict()` for each class.",
              "Add tests `tests/test_models.py`:",
              "Create Player, Token, Obstacle objects; verify fields and serialization round-trip.",
              "Test Player.move() updates position by velocity or given delta.",
              "Acceptance checks:",
              "`python -c \"from src.core.models import Player, Token; p=Player('p1',pos=(0,0)); print(p.score)\"` should print 0 or not error.",
              "`pytest tests/test_models.py` passes.",
              "Commit: `git add src/core/models.py tests/test_models.py && git commit -m \"feat: implement Entity, Player, Token, Obstacle models with serialization\"`"
            ],
            "done": false
          },
          {
            "number": "2.5",
            "description": "Implement game state manager: `src/core/state.py`",
            "details": [
              "Create file `src/core/state.py`.",
              "Implement `GameState` class:",
              "fields: `entities: Dict[str, Entity]`, `time: float`, `score: int`, `level: int`, `rng_seed: Optional[int]`.",
              "methods: `add_entity(entity)`, `remove_entity(entity_id)`, `get_entities_by_tag(tag)`, `advance_time(dt)`, `serialize()/deserialize()`.",
              "implement a deterministic simple RNG using Python's `random.Random(self.rng_seed)` if seed provided for reproducible tests.",
              "Add tests `tests/test_state.py`:",
              "Add entities, advance_time(0.1), verify time increment and entity list updates.",
              "Test serialize/deserialize consistency.",
              "Acceptance checks:",
              "`python -c \"from src.core.state import GameState; s=GameState(); s.advance_time(0.5); print(s.time)\"` prints `0.5`.",
              "`pytest tests/test_state.py` passes.",
              "Commit: `git add src/core/state.py tests/test_state.py && git commit -m \"feat: add GameState manager with entity registry and time advancement\"`"
            ],
            "done": false
          },
          {
            "number": "2.6",
            "description": "Implement collision detection utilities: `src/core/collision.py`",
            "details": [
              "Create file `src/core/collision.py`.",
              "Implement functions:",
              "`collides(a: Entity, b: Entity) -> bool`: circle-circle collision using positions & radii.",
              "`distance(a,b)` returns float.",
              "`broadphase(entities, margin=0.0)` simple n^2 filter returning candidate pairs (document complexity).",
              "Add tests `tests/test_collision.py` that create pairs of entities in-collision and out-of-collision and assert results.",
              "Acceptance checks:",
              "`pytest tests/test_collision.py` passes.",
              "Example run `python -c \"from src.core.collision import collides; print(collides(...))\"` works without error.",
              "Commit: `git add src/core/collision.py tests/test_collision.py && git commit -m \"feat: implement collision detection utilities\"`"
            ],
            "done": false
          },
          {
            "number": "2.7",
            "description": "Implement game rules & token munching logic: `src/core/rules.py`",
            "details": [
              "Create file `src/core/rules.py`.",
              "Implement functions:",
              "`handle_player_token_collision(player: Player, token: Token, state: GameState)`: increase player.score by token.value, mark token.alive=False, optionally increment player.lives or apply effects.",
              "`spawn_token(state: GameState, pos: Vec2, value: int)`: create Token with unique id and add to state.",
              "`tick_rules(state: GameState, dt: float)`: process token expiry, level-up conditions, token spawn timers.",
              "Add tests `tests/test_rules.py`:",
              "Simulate player eating token and assert score increment and token removed.",
              "Test spawn_token adds entity to state.",
              "Acceptance checks:",
              "Run `pytest tests/test_rules.py` and pass.",
              "Manual example via `python -c` demonstrates a player eating token updates score.",
              "Commit: `git add src/core/rules.py tests/test_rules.py && git commit -m \"feat: implement token munching rules and spawn helpers\"`"
            ],
            "done": false
          },
          {
            "number": "2.8",
            "description": "Implement core engine loop & update mechanics: `src/core/engine.py`",
            "details": [
              "Create file `src/core/engine.py`.",
              "Implement `GameEngine` class:",
              "constructor accepts `state: GameState`, `tick_rate: float` default 60.0.",
              "methods: `tick(dt)`: call state.advance_time(dt), update_entities(dt), apply physics (pos += velocity * dt), call `handle_collisions()`, call `tick_rules()`.",
              "`handle_collisions()` uses `broadphase()` then `collides()` and calls appropriate handlers from `rules.py` (e.g., player-token collision).",
              "`step()` convenience method to run a single tick with dt = 1/tick_rate.",
              "include clear error handling and logging via `print()` or `logging`.",
              "Add tests `tests/test_engine.py`:",
              "Set up a GameState with a Player moving into a Token; call engine.tick(dt) and assert token eaten and player's score incremented.",
              "Test that obstacles block movement by checking position after tick if collision should prevent movement (basic expected behavior).",
              "Acceptance checks:",
              "`pytest tests/test_engine.py` passes.",
              "Run a short headless simulation: `python -c \"from src.core.engine import GameEngine; from src.core.state import GameState; s=GameState(); engine=GameEngine(s); engine.step(); print('ok')\"` runs without error.",
              "Commit: `git add src/core/engine.py tests/test_engine.py && git commit -m \"feat: implement GameEngine loop, entity updates, and collision handling\"`"
            ],
            "done": false
          },
          {
            "number": "2.9",
            "description": "Add a headless CLI runner and help text: `src/run.py`",
            "details": [
              "Create file `src/run.py`.",
              "Implement a minimal CLI with argparse:",
              "Options: `--steps N` (number of ticks), `--tick-rate` (Hz), `--seed` (rng seed), `--verbose` flag.",
              "On run, create a default GameState with one Player and 5 Tokens in set positions, run engine for N steps, and print a summary: final score, alive tokens count, elapsed time.",
              "Provide clear error messages if invalid args passed (e.g., negative steps).",
              "Add `if __name__ == \"__main__\":` block so `python src/run.py --help` shows usage.",
              "Add tests `tests/test_run_cli.py`:",
              "Invoke `python -m src.run --steps 1` using subprocess and assert exit code 0 and output contains \"final score\" or similar.",
              "Acceptance checks:",
              "`python src/run.py --help` prints help.",
              "`python src/run.py --steps 2 --seed 42` runs and prints summary.",
              "Commit: `git add src/run.py tests/test_run_cli.py && git commit -m \"feat: add headless CLI runner and help text\"`"
            ],
            "done": false
          },
          {
            "number": "2.10",
            "description": "Add documentation for core model & engine: `docs/core.md`",
            "details": [
              "Create `docs/core.md`.",
              "Contents must include:",
              "Short architecture overview: Entities, GameState, Engine, Rules, Collision.",
              "Public API usage examples: code snippets showing how to instantiate GameState, add entities, run engine.step().",
              "Error messages list and their meanings (e.g., ValueError on invalid radius).",
              "Example of running CLI runner.",
              "Acceptance checks:",
              "Open `docs/core.md` and verify the API example code compiles (copy/paste into REPL) without import errors.",
              "Commit: `git add docs/core.md && git commit -m \"docs: add core model & engine documentation with usage examples\"`"
            ],
            "done": false
          },
          {
            "number": "2.11",
            "description": "Implement test coverage and run full test suite",
            "details": [
              "Add `tests/test_integration.py` that runs an end-to-end headless scenario for ~10 ticks and asserts final state invariants:",
              "Player still alive, score >= 0, no token overlaps.",
              "Run tests: `pytest -q --maxfail=1`.",
              "Acceptance checks:",
              "All tests in `tests/` must pass locally: exit code 0.",
              "Fix failing tests until they pass deterministically (use rng seed where nondeterministic).",
              "Commit: `git add tests/ && git commit -m \"test: add integration test for headless simulation\"`"
            ],
            "done": false
          },
          {
            "number": "2.12",
            "description": "Run formatters and linters and fix issues",
            "details": [
              "Run formatter: `black src/ tests/` and ensure all Python code formatted.",
              "Run linter: `flake8 src/ tests/` and fix any warnings or errors reported (or add inline noqa for acceptable cases with a comment explaining why).",
              "Acceptance checks:",
              "`black --check src/ tests/` returns success after formatting.",
              "`flake8 src/ tests/` exits with code 0.",
              "Commit: `git add -A && git commit -m \"chore: run black and fix flake8 issues\"`"
            ],
            "done": false
          },
          {
            "number": "2.13",
            "description": "Add basic logging and error messages in engine & models",
            "details": [
              "Update `src/core/engine.py` and `src/core/models.py` to use Python `logging` instead of print for important events:",
              "On collision handling, log INFO with player id and token id eaten.",
              "On invalid operations (adding entity with duplicate id), raise ValueError with clear message and log ERROR.",
              "Add tests `tests/test_logging.py`:",
              "Use `caplog` fixture to assert that eating a token logs an INFO line containing player id and token id.",
              "Acceptance checks:",
              "`pytest tests/test_logging.py` passes.",
              "Search logs: `grep -R \"eaten\" -n src/` should find relevant log messages or unit test verifies logs.",
              "Commit: `git add src/core/engine.py src/core/models.py tests/test_logging.py && git commit -m \"feat: add logging and clear error messages in engine and models\"`"
            ],
            "done": false
          },
          {
            "number": "2.14",
            "description": "Final integration commit and developer instructions",
            "details": [
              "Create `DEVELOPER.md` with short instructions:",
              "How to run tests: `pytest`",
              "How to run the CLI: `python src/run.py --steps 100`",
              "How to format/lint: `black src/ tests/ && flake8 src/ tests/`",
              "How to add new entities and rules (reference file paths and function names).",
              "Run a final smoke test: `pytest -q` and `python src/run.py --steps 5 --seed 0`.",
              "Commit: `git add DEVELOPER.md && git commit -m \"docs: add developer instructions and final integration checks\"`"
            ],
            "done": false
          }
        ]
      },
      {
        "number": 3,
        "title": "Map Generation & Token Placement",
        "description": null,
        "steps": [
          {
            "number": "3.1",
            "description": "Create project directories and baseline files for map generation",
            "details": [
              "Create directory structure:",
              "mkdir -p src/mapgen src/mapgen/io src/config scripts tests/unit examples",
              "Create empty module files:",
              "touch src/mapgen/__init__.py src/mapgen/map.py src/mapgen/generator.py src/mapgen/token_placer.py src/mapgen/io/serializer.py",
              "touch src/config/__init__.py config/map.yaml scripts/generate_map.py",
              "Add a minimal package metadata file if not present:",
              "touch pyproject.toml (or update existing) with project name \"token-muncher\" and dev dependencies (black, flake8, pytest)",
              "Acceptance checks:",
              "Verify files exist: ls src/mapgen src/mapgen/io config scripts tests/unit examples",
              "Ensure Python imports succeed: python -c \"import importlib; importlib.import_module('mapgen')\"",
              "Commit baseline structure:",
              "git add src/ config/ scripts/ tests/ examples/ pyproject.toml && git commit -m \"chore: create mapgen directories and baseline files\""
            ],
            "done": false
          },
          {
            "number": "3.2",
            "description": "Implement Map data model in src/mapgen/map.py",
            "details": [
              "Create a Map class with constructor parameters (width:int, height:int, default_tile:str='empty') and attributes grid (2D list), width, height",
              "Add methods:",
              "get(x,y) -> returns tile at coordinates, raise IndexError if out of bounds",
              "set(x,y,tile) -> sets tile, validates coordinates",
              "to_dict() -> serializable representation: {\"width\":..,\"height\":..,\"grid\":[...] }",
              "from_dict(d) -> classmethod to reconstruct Map",
              "to_ascii() -> returns human-readable ASCII string for quick debugging (e.g., '.' for empty, '#' for wall, 'T' for token)",
              "Add inline docstrings and basic type hints for all methods",
              "Add acceptance checks:",
              "In a Python REPL or script run: python -c \"from mapgen.map import Map; m=Map(5,3); assert m.width==5 and m.height==3; m.set(1,1,'wall'); assert m.get(1,1)=='wall'; print(m.to_ascii())\"",
              "Commit the Map model:",
              "git add src/mapgen/map.py && git commit -m \"feat: add Map model with serialization and ASCII export\""
            ],
            "done": false
          },
          {
            "number": "3.3",
            "description": "Implement deterministic random seed utilities in src/mapgen/utils.py",
            "details": [
              "Create src/mapgen/utils.py with functions:",
              "make_rng(seed: Optional[int]) -> returns instance of random.Random seeded by seed",
              "clamp(value, min_value, max_value) helper",
              "Use Python's random module; document deterministic behavior in docstring",
              "Acceptance checks:",
              "Run: python -c \"from mapgen.utils import make_rng; r1=make_rng(42); r2=make_rng(42); assert [r1.random() for _ in range(3)]==[r2.random() for _ in range(3)]\"",
              "Commit utils:",
              "git add src/mapgen/utils.py && git commit -m \"feat: add deterministic RNG and helpers\""
            ],
            "done": false
          },
          {
            "number": "3.4",
            "description": "Implement map generation algorithms in src/mapgen/generator.py",
            "details": [
              "Add two generation functions with signatures:",
              "generate_empty(width:int, height:int, default_tile:str='empty') -> Map",
              "generate_random_walk(width:int, height:int, steps:int, seed:Optional[int]=None, start:Optional[tuple]=None) -> Map",
              "generate_cellular_automata(width:int, height:int, seed:Optional[int]=None, fill_prob:float=0.45, steps:int=4) -> Map",
              "Implementation details:",
              "generate_random_walk: start from center or provided start; use RNG from utils.make_rng; mark visited cells as 'floor' and leave others 'wall' or 'empty' depending on desired theme",
              "generate_cellular_automata: initialize grid with 'wall' with given fill_prob, then apply standard CA rules for a number of steps to produce cave-like maps",
              "All functions must return a Map instance (from src/mapgen/map.py)",
              "Validate input ranges and raise ValueError on invalid parameters (e.g., negative width)",
              "Add docstrings describing parameters and deterministic behavior when seed provided",
              "Acceptance checks:",
              "Create a short script: python -c \"from mapgen.generator import generate_random_walk; m=generate_random_walk(20,10,steps=200,seed=1); print(m.to_ascii())\"",
              "Write a small demonstration that two runs with same seed produce identical ASCII",
              "Commit generator:",
              "git add src/mapgen/generator.py && git commit -m \"feat: implement random_walk and cellular_automata map generators\""
            ],
            "done": false
          },
          {
            "number": "3.5",
            "description": "Implement token placement logic in src/mapgen/token_placer.py",
            "details": [
              "Add public function place_tokens(map_obj: Map, token_count:int, min_distance:int=1, avoid_tiles:List[str]=['wall'], seed:Optional[int]=None) -> List[dict]",
              "Behavior requirements:",
              "Use deterministic RNG (utils.make_rng) when seed provided",
              "Only place tokens on tiles not in avoid_tiles",
              "Enforce min_distance (Manhattan or Euclidean) between tokens; try repeated attempts up to a capped number (e.g., 1000 attempts per token)",
              "Return a list of placement dicts like {\"x\":x,\"y\":y,\"token_id\":n}",
              "Optionally annotate the Map by placing 'token' markers or return placements without modifying Map based on caller preference parameter annotate:bool=False",
              "Edge-case handling:",
              "If not enough valid tiles to place requested token_count with min_distance, raise a RuntimeError with a clear message",
              "Validate token_count non-negative and min_distance >=0",
              "Acceptance checks:",
              "Create a quick runner: python -c \"from mapgen.generator import generate_empty; from mapgen.token_placer import place_tokens; m=generate_empty(10,10); placements=place_tokens(m,5,seed=42); assert len(placements)==5\"",
              "Verify placement respects avoid_tiles by generating a grid full of 'wall' and asserting the function raises RuntimeError",
              "Commit token placement:",
              "git add src/mapgen/token_placer.py && git commit -m \"feat: add token placement with distance constraints and deterministic RNG\""
            ],
            "done": false
          },
          {
            "number": "3.6",
            "description": "Add serialization utilities in src/mapgen/io/serializer.py",
            "details": [
              "Implement functions:",
              "save_map_json(map_obj: Map, path: str) -> writes JSON using map_obj.to_dict()",
              "load_map_json(path: str) -> Map reconstructed via from_dict",
              "save_map_ascii(map_obj: Map, path: str) -> writes ASCII from map_obj.to_ascii()",
              "Ensure error handling:",
              "Raise FileNotFoundError/IOError when unable to write, with user-friendly error messages",
              "Validate the path directory exists or create with os.makedirs(os.path.dirname(path), exist_ok=True)",
              "Acceptance checks:",
              "Run: python -c \"from mapgen.generator import generate_empty; from mapgen.io.serializer import save_map_json, load_map_json; m=generate_empty(4,3); save_map_json(m,'examples/maps/test_map.json'); m2=load_map_json('examples/maps/test_map.json'); assert m2.width==4\"",
              "Confirm files created under examples/maps/",
              "Commit serializer:",
              "git add src/mapgen/io/serializer.py examples/maps && git commit -m \"feat: add map JSON/ASCII serialization utilities and example output\""
            ],
            "done": false
          },
          {
            "number": "3.7",
            "description": "Create a CLI script to generate maps and place tokens: scripts/generate_map.py",
            "details": [
              "Implement CLI parsing (argparse) supporting flags:",
              "--width, --height, --generator (choices: empty, random_walk, cellular), --steps (for generators), --tokens (int), --min-distance, --seed, --out (output path), --format (json|ascii), --annotate (flag)",
              "Behavior:",
              "Validate arguments and print helpful error messages (exit code 2 on invalid args)",
              "Call generator functions and token_placer with provided args",
              "Save output using serializer functions (JSON or ASCII)",
              "Print a one-line summary after creation: \"Map generated: 20x10, tokens placed: 5, saved to examples/maps/out.json\"",
              "Include usage examples in --help text",
              "Acceptance checks:",
              "Run end-to-end sample: python scripts/generate_map.py --width 20 --height 10 --generator random_walk --steps 300 --tokens 5 --seed 42 --out examples/maps/cli_map.json --format json",
              "Verify examples/maps/cli_map.json exists and contains expected width/height and a placements list if annotated",
              "Commit CLI:",
              "git add scripts/generate_map.py && git commit -m \"feat: add CLI for map generation and token placement\""
            ],
            "done": false
          },
          {
            "number": "3.8",
            "description": "Write unit tests in tests/unit/ for generator, token placement, serializer, and map model",
            "details": [
              "Create files:",
              "tests/unit/test_map.py: tests for Map.get/set, to_dict/from_dict, out-of-bounds errors",
              "tests/unit/test_generator.py: tests for deterministic output with seed, parameter validation, small map outputs",
              "tests/unit/test_token_placer.py: tests for correct number of placements, min_distance enforcement, error when impossible",
              "tests/unit/test_serializer.py: tests for save/load json and ascii round-trip",
              "Test details:",
              "Use pytest; each test should assert deterministic seeds produce same results, and that invalid parameters raise proper exceptions (ValueError/RuntimeError)",
              "Use tmp_path fixture for file IO to avoid polluting repo",
              "Add acceptance checks:",
              "Run: pytest tests/unit/ -q and ensure all tests pass",
              "Commit tests:",
              "git add tests/unit/ && git commit -m \"test: add unit tests for map generation, token placement, and serialization\""
            ],
            "done": false
          },
          {
            "number": "3.9",
            "description": "Add linters, formatters, and run quality checks",
            "details": [
              "Add or update pyproject.toml to include black and flake8 config sections (line-length 88 or project preference)",
              "Run formatting and linting:",
              "black src/ tests/ scripts/",
              "flake8 src/ tests/ scripts/",
              "Fix any lint issues flagged by flake8 (unused imports, style errors)",
              "Re-run pytest to ensure formatting fixes didn't break tests:",
              "pytest tests/unit/ -q",
              "Acceptance checks:",
              "black should report \"reformatted\" or \"already formatted\" and flake8 should exit 0",
              "pytest should pass",
              "Commit formatting/linting changes:",
              "git add . && git commit -m \"chore: format code with black and fix flake8 issues\""
            ],
            "done": false
          },
          {
            "number": "3.10",
            "description": "Add documentation and usage examples",
            "details": [
              "Create docs file docs/map_generation.md with:",
              "Overview of available generators and when to use each",
              "Explanation of token placement rules (avoid_tiles, min_distance)",
              "CLI usage examples (copy three example commands and expected outputs)",
              "Example JSON structure showing keys produced by serializer and placement examples",
              "Update README.md root section \"Map Generation\" with short usage snippet and link to docs/map_generation.md",
              "Acceptance checks:",
              "Open docs/map_generation.md and confirm it contains CLI examples and JSON example",
              "Ensure README contains minimal instructions for running scripts/generate_map.py",
              "Commit docs:",
              "git add docs/map_generation.md README.md && git commit -m \"docs: document map generators, token placement rules, and CLI usage\""
            ],
            "done": false
          },
          {
            "number": "3.11",
            "description": "Add integration test (end-to-end) and CI-friendly script",
            "details": [
              "Create tests/integration/test_end_to_end.py that:",
              "Runs the CLI script using subprocess.run with a temp output path",
              "Asserts exit code 0, checks output file exists, loads JSON and verifies width/height and token count",
              "Runs with different generator options (random_walk, cellular) using parameterize",
              "Ensure test is deterministic by passing seed values",
              "Local acceptance check:",
              "pytest tests/integration/test_end_to_end.py -q",
              "Commit integration test:",
              "git add tests/integration/test_end_to_end.py && git commit -m \"test: add end-to-end CLI integration tests\""
            ],
            "done": false
          },
          {
            "number": "3.12",
            "description": "Add error messages, input validation and edge-case tests",
            "details": [
              "Update functions to include clear, user-friendly exception messages:",
              "Map methods: \"Coordinates out of bounds: (x,y)\"",
              "Generators: \"Invalid parameter X: reason\"",
              "Token placer: \"Unable to place N tokens given constraints: available tiles M < N\"",
              "Serializer: \"Could not write to path: <path> (reason)\"",
              "Add tests in tests/unit/test_errors.py to assert the above messages are present when the corresponding exceptions are raised",
              "Acceptance checks:",
              "Run pytest tests/unit/test_errors.py and verify assertions on exception messages",
              "Commit error handling changes:",
              "git add src/ tests/unit/test_errors.py && git commit -m \"fix: improve validation messages and add tests for error cases\""
            ],
            "done": false
          },
          {
            "number": "3.13",
            "description": "Provide examples directory and sample maps for manual QA",
            "details": [
              "Generate and save three sample maps using the CLI into examples/maps/:",
              "examples/maps/sample_empty.json (empty generator)",
              "examples/maps/sample_random.json (random_walk with seed 100)",
              "examples/maps/sample_cave.json (cellular_automata with seed 200)",
              "Include a README snippet in examples/maps/README.md describing how to view ASCII or JSON and how to re-generate",
              "Acceptance checks:",
              "Confirm files exist and can be loaded by serializer: python -c \"from mapgen.io.serializer import load_map_json; load_map_json('examples/maps/sample_random.json')\"",
              "Commit sample maps and examples README:",
              "git add examples/maps/ && git commit -m \"docs: add sample maps and examples readme for QA\""
            ],
            "done": false
          },
          {
            "number": "3.14",
            "description": "Final verification: run full test suite and finalize commit",
            "details": [
              "Run formatting, linting, and tests one more time:",
              "black src/ tests/ scripts/",
              "flake8 src/ tests/ scripts/",
              "pytest -q",
              "If CI config exists, run local linters/formatters used in CI (e.g., pre-commit)",
              "Final acceptance:",
              "All tests pass, flake8 exits 0, black reports clean",
              "Example CLI command from README works end-to-end and produces expected output file",
              "Final commit (if any remaining unstaged changes):",
              "git add . && git commit -m \"chore: finalize map generation and token placement phase (format/tests/docs)\""
            ],
            "done": false
          }
        ]
      },
      {
        "number": 4,
        "title": "Rendering & Input Layer",
        "description": null,
        "steps": [
          {
            "number": "4.1",
            "description": "Create frontend project scaffold at `src/frontend/`",
            "details": [
              "Create directory structure:",
              "`mkdir -p src/frontend/src/components src/frontend/src/api src/frontend/src/styles tests/frontend`",
              "Initialize Node project and add TypeScript + React tooling:",
              "Run: `cd src/frontend && npm init -y`",
              "Run: `npm install react react-dom axios`",
              "Run dev / tooling deps: `npm install --save-dev typescript @types/react @types/react-dom eslint prettier eslint-config-prettier eslint-plugin-react jest @testing-library/react @testing-library/jest-dom ts-jest @types/jest`",
              "Create TypeScript config: `src/frontend/tsconfig.json` with \"jsx\": \"react-jsx\" and \"strict\": true",
              "Add basic npm scripts to `src/frontend/package.json`:",
              "\"start\": \"webpack serve\" (or your dev server), \"build\": \"webpack --mode production\", \"test\": \"jest\", \"lint\": \"eslint src/\", \"format\": \"prettier --write 'src/**/*.{ts,tsx,js,json,css}'\"",
              "Acceptance checks:",
              "`cd src/frontend && npm run test` runs (may initially report no tests but should not error)",
              "`ls src/frontend/src` shows `components/`, `api/`, `styles/`",
              "Commit scaffold:",
              "`git add src/frontend/package.json src/frontend/tsconfig.json` && `git commit -m \"chore: add frontend scaffold and dev tooling\"`"
            ],
            "done": false
          },
          {
            "number": "4.2",
            "description": "Implement canvas-based renderer component `src/frontend/src/components/TokenRenderer.tsx`",
            "details": [
              "Create file `src/frontend/src/components/TokenRenderer.tsx`",
              "Implement a React functional component TokenRenderer with these specifics:",
              "Props: tokens: Array<{ id: string; x: number; y: number; size?: number; color?: string }>, width: number, height: number, selectedId?: string, scale?: number, onTokenClick?: (id: string) => void",
              "Use an HTMLCanvasElement via `useRef` and draw in `useEffect` whenever tokens/scale/selectedId change",
              "Implement draw loop:",
              "clear canvas with devicePixelRatio handling",
              "for each token draw a filled circle at (x*scale,y*scale) with size*scale, color fallback `#333`",
              "if token.id === selectedId draw stroke highlight",
              "Add pointer event handling: map canvas click coordinates to logical coordinates (account for scale and canvas bounding rect) and call `onTokenClick` for the nearest token within radius",
              "Export helper function `export function getNearestToken(tokens, x, y, threshold)` inside the same file for unit testing",
              "Add inline JSDoc comments for exported props and helper",
              "Acceptance checks:",
              "Import TokenRenderer in a small story or App render \u2014 canvas appears, tokens array of at least 3 shows 3 circles",
              "Clicking near a circle triggers `onTokenClick` (can be confirmed by console.log)",
              "Commit renderer:",
              "`git add src/frontend/src/components/TokenRenderer.tsx` && `git commit -m \"feat(frontend): add canvas TokenRenderer component with click handling\"`"
            ],
            "done": false
          },
          {
            "number": "4.3",
            "description": "Implement input layer component `src/frontend/src/components/InputBar.tsx`",
            "details": [
              "Create file `src/frontend/src/components/InputBar.tsx`",
              "Implement a controlled React component InputBar with these specifics:",
              "UI includes: single-line text input, submit button, and a clear button",
              "Props: onSubmit: (text: string) => Promise<void> | void, placeholder?: string",
              "Behavior:",
              "Enter key submits (handle keyboard Enter)",
              "Disable submit button when input is empty or when `submitting` state true",
              "Show inline validation error if text length > 500 and prevent submit",
              "Show accessible labels (`aria-label`) and keyboard focus styles",
              "When submitting:",
              "set `submitting` true, await onSubmit(text), reset input on success, show an error message if promise rejects",
              "Provide small helper `sanitizeInput(text)` exported for unit tests",
              "Acceptance checks:",
              "Typing and pressing Enter calls `onSubmit` with exact typed text",
              "Disabled state and validation for >500 chars works",
              "Error message appears if `onSubmit` rejects (simulate with Promise.reject)",
              "Commit input component:",
              "`git add src/frontend/src/components/InputBar.tsx` && `git commit -m \"feat(frontend): add InputBar with validation and keyboard submit\"`"
            ],
            "done": false
          },
          {
            "number": "4.4",
            "description": "Implement controls component for zoom/pan/play `src/frontend/src/components/Controls.tsx`",
            "details": [
              "Create file `src/frontend/src/components/Controls.tsx`",
              "Implement a React component Controls with these specifics:",
              "Props: scale: number, onScaleChange: (newScale:number) => void, onCenter: () => void, playing: boolean, onTogglePlay: () => void",
              "UI contains: range slider for scale (min 0.5 to 3.0 step 0.1), \"Center\" button, Play/Pause toggle button, keyboard shortcuts shown in tooltip",
              "Behavior:",
              "Slider changes call onScaleChange with the numeric value",
              "\"Center\" emits onCenter",
              "Play/Pause toggles emit onTogglePlay",
              "Provide accessible labels and keyboard focus",
              "Add local display of current scale (e.g., \"Scale: 1.2x\")",
              "Acceptance checks:",
              "Moving slider updates displayed value and triggers onScaleChange with correct number",
              "Buttons call respective callbacks on click",
              "Commit controls:",
              "`git add src/frontend/src/components/Controls.tsx` && `git commit -m \"feat(frontend): add Controls for zoom, center and play/pause\"`"
            ],
            "done": false
          },
          {
            "number": "4.5",
            "description": "Wire renderer, input, and controls into app shell files `src/frontend/src/App.tsx` and `src/frontend/src/index.tsx`",
            "details": [
              "Create `src/frontend/src/App.tsx`",
              "Import TokenRenderer, InputBar, Controls",
              "Maintain state: tokens (array), scale (number), selectedId, playing (boolean)",
              "Implement behavior:",
              "onSubmit from InputBar parses the input into token objects (e.g., split by whitespace into tokens with generated ids and random positions) and calls a local function `addTokens`",
              "addTokens appends tokens to state and triggers a short animation if playing",
              "onTokenClick from TokenRenderer sets selectedId and passes to a mock API client (see next step) to persist selection",
              "onScaleChange updates scale and passes to TokenRenderer",
              "onCenter recenters tokens by adjusting a translation value (store in state if needed)",
              "Export App as default",
              "Create `src/frontend/src/index.tsx`",
              "Bootstrap React root, render <App /> into `#root`",
              "Ensure a minimal `public/index.html` exists with `div#root`",
              "Acceptance checks:",
              "Running dev server displays InputBar, Controls, and an empty canvas",
              "Submitting sample text populates tokens on the canvas",
              "Clicking a token marks it selected",
              "Commit shell:",
              "`git add src/frontend/src/App.tsx src/frontend/src/index.tsx public/index.html` && `git commit -m \"feat(frontend): wire TokenRenderer, InputBar and Controls into App shell\"`"
            ],
            "done": false
          },
          {
            "number": "4.6",
            "description": "Implement frontend API client `src/frontend/src/api/client.ts`",
            "details": [
              "Create file `src/frontend/src/api/client.ts`",
              "Implement functions:",
              "`export async function postTokens(tokens: Token[]): Promise<{status:string, ids:string[]}>` \u2014 use `axios.post('/api/tokens', tokens)`; fallback to a local mock that returns generated ids if no server",
              "`export async function patchSelection(tokenId: string): Promise<void>` \u2014 call `axios.patch('/api/tokens/selection', { id: tokenId })` or mock",
              "Add a `isMock` flag exported for tests and dev mode detection",
              "Add small retry logic: try once, on network error log and return mock result",
              "Acceptance checks:",
              "`import { postTokens }` in App and call with tokens \u2014 ensure resolved Promise returns ids and App uses them",
              "Unit test for mock path returns expected shape",
              "Commit API client:",
              "`git add src/frontend/src/api/client.ts` && `git commit -m \"feat(frontend): add API client with mock fallback for tokens\"`"
            ],
            "done": false
          },
          {
            "number": "4.7",
            "description": "Write unit tests for renderer and input in `tests/frontend/`",
            "details": [
              "Create tests:",
              "`tests/frontend/TokenRenderer.test.tsx`",
              "Test `getNearestToken` helper: supply tokens array and assert correct id for given coordinates and threshold",
              "Render TokenRenderer with a set of tokens and use RTL `fireEvent.click` on canvas coordinates (use mocking of bounding client rect) to assert `onTokenClick` called with expected id",
              "`tests/frontend/InputBar.test.tsx`",
              "Test controlled input behavior, Enter key submission, button disabled state, and validation >500 chars shows error",
              "Configure Jest `jest.config.js` in `src/frontend/` and add `tests` path",
              "Run tests:",
              "`cd src/frontend && npm run test`",
              "Acceptance checks:",
              "Tests pass (or if snapshots not used, at least tests run and assert logic)",
              "Commit tests:",
              "`git add tests/frontend/ src/frontend/jest.config.js` && `git commit -m \"test(frontend): add unit tests for TokenRenderer and InputBar\"`"
            ],
            "done": false
          },
          {
            "number": "4.8",
            "description": "Add styles and accessibility improvements `src/frontend/src/styles/`",
            "details": [
              "Create `src/frontend/src/styles/renderer.css` and `src/frontend/src/styles/input.css`",
              "Style details:",
              "Canvas wrapper with keyboard focus outline, ensure high-contrast token default colors, responsive layout (canvas grows to container)",
              "InputBar styles: visible focus ring, disabled button style, error message style with `role=\"alert\"`",
              "Controls: larger hit areas for buttons, aria-label attributes",
              "Wire styles into components via import statements (e.g., `import '../styles/renderer.css'`)",
              "Acceptance checks:",
              "Tab navigation reaches input, slider and buttons; focus ring visible",
              "Error message has `role=\"alert\"` and is read by screen readers (manual check or a11y lint)",
              "Commit styling:",
              "`git add src/frontend/src/styles/*.css` && `git commit -m \"feat(frontend): add styles and accessibility improvements\"`"
            ],
            "done": false
          },
          {
            "number": "4.9",
            "description": "Add end-to-end smoke test script and local QA instructions `tests/frontend/e2e/smoke.test.js`",
            "details": [
              "Create `tests/frontend/e2e/smoke.test.js` (can be a simple Puppeteer or Playwright test)",
              "Test plan:",
              "Start local dev server (assume `npm run start`), navigate to `http://localhost:3000`",
              "Type a sample string into InputBar, submit, wait for tokens to appear on canvas, click one token and assert selection UI updates",
              "Add npm script: `\"e2e:smoke\": \"node tests/frontend/e2e/smoke-runner.js\"` or use Playwright if already installed",
              "Provide `tests/frontend/e2e/smoke-runner.js` that:",
              "Uses Playwright to launch browser, navigate, perform steps above, and exit non-zero if any assertion fails",
              "Acceptance checks:",
              "`cd src/frontend && npm run e2e:smoke` returns exit code 0 after successful interactions",
              "Commit e2e:",
              "`git add tests/frontend/e2e && git commit -m \"test(frontend): add e2e smoke test for rendering and input flow\"`"
            ],
            "done": false
          },
          {
            "number": "4.10",
            "description": "Run formatters and linters, fix issues",
            "details": [
              "Run formatting:",
              "`cd src/frontend && npm run format`",
              "Run linter:",
              "`cd src/frontend && npm run lint`",
              "Fix any lint errors (e.g., missing prop types, unused imports, accessibility warnings)",
              "Run TypeScript check:",
              "`cd src/frontend && npx tsc --noEmit`",
              "Run unit tests:",
              "`cd src/frontend && npm run test`",
              "Acceptance checks:",
              "`npm run format` makes no further changes after re-run",
              "`npm run lint` exits 0 (or has only justified warnings)",
              "`npx tsc --noEmit` exits 0",
              "All unit tests pass",
              "Commit lint/types/fix changes:",
              "`git add src/frontend/ && git commit -m \"chore(frontend): format, lint fixes and type-check\"`"
            ],
            "done": false
          },
          {
            "number": "4.11",
            "description": "Add user-facing help modal and inline help text `src/frontend/src/components/HelpModal.tsx` and update README",
            "details": [
              "Create file `src/frontend/src/components/HelpModal.tsx`",
              "Implement a modal component with:",
              "Title \"Token Muncher \u2014 Help\"",
              "Short instructions: how to input tokens, keyboard shortcuts (Enter to submit, arrow keys to pan, +/- to zoom), explanation of controls and error messages",
              "Close button and accessible roles (`role=\"dialog\"`, aria-modal)",
              "Add a \"Help\" button to `App.tsx` that toggles the modal",
              "Update `README.md` at repo root with a \"Frontend: Rendering & Input\" section:",
              "Describe how to run dev server, run tests, run e2e smoke test, and common troubleshooting steps",
              "Add examples of valid input and expected result",
              "Acceptance checks:",
              "Clicking Help opens modal; modal content matches README and is accessible",
              "README step commands work when followed",
              "Commit docs and help:",
              "`git add src/frontend/src/components/HelpModal.tsx README.md` && `git commit -m \"docs(frontend): add Help modal and README usage notes\"`"
            ],
            "done": false
          },
          {
            "number": "4.12",
            "description": "Error handling, metrics hooks, and graceful degradation",
            "details": [
              "Update `src/frontend/src/components/TokenRenderer.tsx` and `src/frontend/src/App.tsx` to add robust error paths:",
              "TokenRenderer: catch canvas drawing exceptions and show fallback overlay `<div>` with visible error message if draw fails",
              "API client: add `onError` callback usage in App to surface network/API errors to the user via an `Alert` component",
              "Implement `src/frontend/src/components/Alert.tsx` for consistent error/success messaging; use `role=\"alert\"`",
              "Add simple metrics hooks:",
              "`src/frontend/src/telemetry.ts` with functions `logEvent(name,payload)` that currently console.logs and can be swapped with a real telemetry sink later",
              "Call `logEvent('token_submit', { count })` when tokens submitted and `logEvent('token_click', { id })` on token click",
              "Acceptance checks:",
              "Forcing an exception in draw triggers fallback overlay",
              "API network failure shows Alert with clear message and a \"Retry\" button",
              "`logEvent` calls appear in console when actions occur",
              "Commit error handling and telemetry:",
              "`git add src/frontend/src/components/Alert.tsx src/frontend/src/telemetry.ts src/frontend/src/components/TokenRenderer.tsx src/frontend/src/App.tsx` && `git commit -m \"fix(frontend): add error handling, alert UI and telemetry hooks\"`"
            ],
            "done": false
          },
          {
            "number": "4.13",
            "description": "Final verification and release notes",
            "details": [
              "Run full verification:",
              "Start dev server: `cd src/frontend && npm run start` and manually verify:",
              "InputBar accepts input and submits tokens",
              "TokenRenderer draws tokens correctly and selection highlights work",
              "Controls change scale and center works",
              "Help modal displays and README instructions match behavior",
              "Run automated checks: `npm run format`, `npm run lint`, `npx tsc --noEmit`, `npm run test`, `npm run e2e:smoke`",
              "Create a changelog entry in `CHANGELOG.md` summarizing the phase:",
              "Add entry: \"Phase 4 \u2014 Rendering & Input Layer: added TokenRenderer, InputBar, Controls, API client, tests, accessibility and help modal\"",
              "Final commit for release notes:",
              "`git add CHANGELOG.md` && `git commit -m \"docs: add changelog entry for Rendering & Input Layer (Phase 4)\"`"
            ],
            "done": false
          }
        ]
      },
      {
        "number": 5,
        "title": "Player Mechanics, Items & Tokens",
        "description": null,
        "steps": [
          {
            "number": "5.1",
            "description": "Create item and token model files",
            "details": [
              "Create file src/models/item.py and implement a dataclass Item with fields: id: str, name: str, description: str, token_cost: int, effect: dict",
              "Add methods in Item: to_dict(self) -> dict and @classmethod from_dict(cls, data: dict) -> \"Item\"; validate types and raise ValueError on invalid input",
              "Create file src/models/token.py and implement a dataclass Token with fields: owner_id: str, amount: int, currency: str = \"TOK\"; add methods add(amount), spend(amount) that validate non-negative results and raise ValueError with clear messages (\"Not enough tokens\", \"Invalid amount\")",
              "Add module-level __all__ exports in both files: __all__ = [\"Item\"] / [\"Token\"]",
              "Acceptance checks: import both classes in a Python REPL and run Item.from_dict({\"id\":\"m1\",\"name\":\"Munch\",\"description\":\"tasty\",\"token_cost\":5,\"effect\":{}}) and Token(\"p1\", 10).spend(5) should reduce amount to 5; spending 6 should raise ValueError",
              "Commit: git add src/models/item.py src/models/token.py && git commit -m \"feat: add Item and Token models with validation\""
            ],
            "done": false
          },
          {
            "number": "5.2",
            "description": "Create player and inventory model files",
            "details": [
              "Create file src/models/inventory.py with class Inventory:",
              "attributes: capacity: int, items: dict[str, int] (item_id -> count)",
              "methods: add_item(item_id: str, count: int = 1) -> None (respect capacity), remove_item(item_id: str, count: int = 1) -> None, has_item(item_id) -> bool, item_count(item_id) -> int",
              "add clear, list_items -> dict, and raise ValueError on invalid operations (e.g., removing non-existent item)",
              "Create file src/models/player.py with class Player:",
              "attributes: id: str, name: str, hp: int, inventory: Inventory, tokens: int",
              "methods: to_dict()/from_dict(), earn_tokens(amount), spend_tokens(amount) that call Token-style validation, apply_item_effect(item: Item) which alters player state (hp, tokens) according to item.effect",
              "Add __repr__ or __str__ for readable output",
              "Add unit-level docstrings for all public methods and clear error messages like \"Inventory full\" and \"Item not in inventory\"",
              "Acceptance checks: instantiate Inventory(capacity=2) and Player(\"p1\",\"A\",10, inventory, tokens=0) then add/remove items and call player.earn_tokens(5) to assert tokens == 5",
              "Commit: git add src/models/inventory.py src/models/player.py && git commit -m \"feat: add Player and Inventory models with basic operations\""
            ],
            "done": false
          },
          {
            "number": "5.3",
            "description": "Implement item registry and default item definitions",
            "details": [
              "Create file src/game/item_registry.py",
              "Implement ItemRegistry class with methods register(item: Item), get(item_id: str) -> Item | None, list_items() -> list[Item]",
              "Add a function load_defaults_from_json(file_path: str) that loads a JSON file into Item objects and registers them",
              "Create data file data/items.json containing at least 4 example items (munch_cookie, small_heal, speed_boost, token_pack) with id, name, description, token_cost, and effect json (e.g., {\"hp\": +5}, {\"tokens\": +10})",
              "Ensure registry enforces unique item ids and raises ValueError on duplicates",
              "Acceptance checks: run a short script or REPL: from src.game.item_registry import ItemRegistry; r=ItemRegistry(); r.load_defaults_from_json(\"data/items.json\"); assert len(r.list_items()) >= 4",
              "Commit: git add src/game/item_registry.py data/items.json && git commit -m \"feat: add ItemRegistry and default items JSON\""
            ],
            "done": false
          },
          {
            "number": "5.4",
            "description": "Implement game mechanics API (use_item, pickup_item, tokens)",
            "details": [
              "Create file src/game/mechanics.py",
              "Implement functions:",
              "pickup_item(player: Player, item_id: str, registry: ItemRegistry) -> None: validate size, add to inventory, raise ValueError with \"Inventory full\" message if full",
              "use_item(player: Player, item_id: str, registry: ItemRegistry) -> dict: check item in inventory, remove one, apply effect via Player.apply_item_effect or directly; return a result dict with keys: success, message, applied_effects",
              "earn_tokens(player: Player, amount: int) -> None and spend_tokens(player: Player, amount: int) -> None (reuse player methods)",
              "Add clear exception messages: \"Item not found\", \"Item not in inventory\", \"Insufficient tokens\"",
              "Write inline docstrings describing preconditions and return values",
              "Add small helper function effect_applicator(player, effect_dict) implementing common effects (hp change, token change, temporary buffs represented as metadata)",
              "Acceptance checks: write a quick script that registers default items, creates a player, picks up munch_cookie, uses munch_cookie, and asserts hp or tokens changed according to data/items.json",
              "Commit: git add src/game/mechanics.py && git commit -m \"feat: implement core game mechanics: pickup, use_item, token ops\""
            ],
            "done": false
          },
          {
            "number": "5.5",
            "description": "Add simple persistence for player state (JSON)",
            "details": [
              "Create file src/storage/player_store.py",
              "Implement functions save_player(player: Player, path: str) -> None and load_player(path: str) -> Player",
              "Use Player.to_dict()/from_dict() and json.dump/json.load; ensure atomic write (write to tmp file then rename) to avoid corruption",
              "Validate loaded data and raise ValueError if required fields missing",
              "Add acceptance checks: create Player, save to tmp file tests/tmp_player.json, load it back and assert equality of id, name, hp, tokens, and inventory contents",
              "Commit: git add src/storage/player_store.py && git commit -m \"feat: add JSON persistence for Player state\""
            ],
            "done": false
          },
          {
            "number": "5.6",
            "description": "Implement CLI for interacting with players, items, and tokens",
            "details": [
              "Create file src/cli.py using argparse (or click if project already uses it; choose argparse for minimal deps)",
              "Provide commands/subcommands: create-player, show-player, list-items, pickup-item, use-item, earn-tokens, spend-tokens",
              "Each command should accept flags: --player-file PATH (path to JSON store), --item-id ID, --amount N, --name NAME",
              "On errors, print user-friendly messages and exit with non-zero status; provide --help text and usage examples in help output",
              "Implement CLI to call into the mechanics, registry, and player_store modules; default to data/items.json for registry",
              "Acceptance checks: run example commands:",
              "python -m src.cli create-player --name \"Alice\" --player-file /tmp/alice.json",
              "python -m src.cli list-items",
              "python -m src.cli pickup-item --player-file /tmp/alice.json --item-id munch_cookie",
              "Expect readable console outputs and saved /tmp/alice.json updated",
              "Commit: git add src/cli.py && git commit -m \"feat: add CLI for player and item interactions\""
            ],
            "done": false
          },
          {
            "number": "5.7",
            "description": "Write unit tests for models and mechanics",
            "details": [
              "Create tests/unit/test_models_item_token.py:",
              "Test Item.from_dict/to_dict roundtrip, invalid field types raising ValueError, Token.spend/add validations",
              "Create tests/unit/test_player_inventory.py:",
              "Test Inventory.add/remove behavior, capacity enforcement, Player.earn_tokens/spend_tokens, Player.apply_item_effect effects (hp increase/decrease)",
              "Create tests/unit/test_mechanics.py:",
              "Test pickup_item and use_item flows with a mock ItemRegistry (or using the real one with data/items.json), including error cases (using an item not in inventory, full inventory)",
              "Each test file should include pytest fixtures for a fresh Player and registry; use assertions to check state after each operation",
              "Run tests locally: pytest tests/unit -q",
              "Acceptance checks: All unit tests pass (exit code 0)",
              "Commit: git add tests/unit && git commit -m \"test: add unit tests for models, inventory, and mechanics\""
            ],
            "done": false
          },
          {
            "number": "5.8",
            "description": "Write integration tests for CLI flows",
            "details": [
              "Create tests/integration/test_cli_flow.py:",
              "Use subprocess.run or pytest's tmp_path to run the CLI commands created in 5.6 in sequence:",
              "create-player -> list-items -> pickup-item -> use-item -> earn-tokens -> spend-tokens",
              "Assert each subprocess returns exit code 0 (or non-zero when expecting an error) and that the saved player JSON matches expected state after operations",
              "Include negative test: try to spend more tokens than the player has and assert non-zero exit code and error message contains \"Not enough tokens\"",
              "Run integration tests: pytest tests/integration -q",
              "Acceptance checks: All integration tests pass",
              "Commit: git add tests/integration && git commit -m \"test: add integration tests for CLI flows and error handling\""
            ],
            "done": false
          },
          {
            "number": "5.9",
            "description": "Add documentation for player mechanics, items, and tokens",
            "details": [
              "Create docs/player.md:",
              "Describe Player fields, inventory behavior, example JSON saved player file, and sample code snippets for using the Player API",
              "Create docs/items.md:",
              "Document Item schema, example items.json format, and explain effect keys (hp, tokens, duration for buffs)",
              "Create docs/cli.md:",
              "Document each CLI command, flags, and example command sequences with expected console output and saved file changes",
              "Add README additions: update README.md root or docs/index.md to include short guide and link to these docs",
              "Acceptance checks: open docs files and ensure examples match actual CLI commands and item effect keys used in code",
              "Commit: git add docs && git commit -m \"docs: add player, items, and CLI documentation\""
            ],
            "done": false
          },
          {
            "number": "5.10",
            "description": "Run formatters, linters, and fix style issues",
            "details": [
              "Run formatting: black src/ tests/ && echo \"black run complete\"",
              "Run linting: flake8 src/ tests/ --max-line-length=88 and fix any warnings/errors raised",
              "Run type checks if project uses mypy: mypy src/ (optional; only if mypy configured)",
              "Re-run unit tests: pytest tests/unit -q",
              "Acceptance checks: black makes no uncommitted changes, flake8 reports zero errors, and unit tests still pass",
              "Commit style fixes: git add . && git commit -m \"chore: apply code formatting and lint fixes\""
            ],
            "done": false
          },
          {
            "number": "5.11",
            "description": "Add CI job config (optional but recommended)",
            "details": [
              "Create .github/workflows/ci.yml with steps:",
              "Checkout, set up Python, install dependencies (pip install -r requirements.txt or pip install pytest flake8 black), run black --check, flake8, pytest",
              "Ensure the workflow runs on push and pull_request",
              "Acceptance checks: validate YAML with an online linter or run act (if available) locally to simulate",
              "Commit: git add .github/workflows/ci.yml && git commit -m \"chore: add CI workflow for linting and tests\""
            ],
            "done": false
          },
          {
            "number": "5.12",
            "description": "Final acceptance test and top-level integration script",
            "details": [
              "Create script scripts/run_demo.sh (or .py) that:",
              "Creates a demo player file in tmp, loads default items, performs a pickup and use sequence, prints final player JSON",
              "Contains comments on expected outputs and exit codes",
              "Make the script executable: chmod +x scripts/run_demo.sh",
              "Run the demo script locally and verify the printed final player JSON shows inventory and token/hp updates according to the items used",
              "Commit demo script and mark as chore: git add scripts/run_demo.sh && git commit -m \"chore: add demo script for manual integration testing\""
            ],
            "done": false
          },
          {
            "number": "5.13",
            "description": "Update project-level metadata and developer notes",
            "details": [
              "Modify or create CONTRIBUTORS.md (or docs/development.md) with instructions to run tests and linters:",
              "Include commands: python -m venv .venv && . .venv/bin/activate, pip install -r requirements.txt, black src/ tests/, flake8 src/ tests/, pytest tests/",
              "Add note about item JSON location (data/items.json) and player persistence path usage",
              "Acceptance checks: a developer following the docs should be able to run black, flake8, and pytest with no missing commands",
              "Commit: git add CONTRIBUTORS.md || git add docs/development.md && git commit -m \"docs: add developer setup and test instructions for player mechanics phase\""
            ],
            "done": false
          }
        ]
      },
      {
        "number": 6,
        "title": "Enemy AI & Pathfinding",
        "description": null,
        "steps": [
          {
            "number": "6.1",
            "description": "Create directory structure for AI and pathfinding",
            "details": [
              "Create directories: src/pathfinding/, src/ai/, src/entities/, src/debug/, tests/unit/, docs/",
              "CLI: mkdir -p src/pathfinding src/ai src/entities src/debug tests/unit docs",
              "Create empty __init__.py files to treat modules as packages:",
              "Files: src/pathfinding/__init__.py, src/ai/__init__.py, src/entities/__init__.py, src/debug/__init__.py",
              "CLI: touch src/pathfinding/__init__.py src/ai/__init__.py src/entities/__init__.py src/debug/__init__.py",
              "Create placeholder files for later implementation:",
              "Files: src/pathfinding/grid.py, src/pathfinding/a_star.py, src/ai/fsm.py, src/ai/enemy_controller.py, src/entities/enemy_entity.py, src/entities/player_entity.py, src/debug/path_debug.py",
              "CLI: touch those paths",
              "Acceptance check: Verify directories and files exist (ls src/pathfinding src/ai src/entities src/debug tests/unit docs) and __init__.py files are present."
            ],
            "done": false
          },
          {
            "number": "6.2",
            "description": "Implement grid-based map representation in src/pathfinding/grid.py",
            "details": [
              "Create class Grid with signature: class Grid: def __init__(self, width: int, height: int, walkable: Optional[Iterable[Tuple[int,int]]] = None, diagonal: bool = False)",
              "Implement methods:",
              "is_walkable(x,y) -> bool \u2014 returns False if out of bounds or tile is blocked",
              "neighbors(x,y) -> Iterable[Tuple[int,int]] \u2014 returns adjacent (and optional diagonal) walkable coordinates",
              "cost(from_x,from_y,to_x,to_y) -> float \u2014 returns movement cost (default 1 or sqrt(2) for diagonal)",
              "to_index(x,y) and from_index(i) helpers (optional but helpful for tests)",
              "Add docstrings and simple type hints",
              "Add inline input validation: raise ValueError if width/height <= 0",
              "Add unit test skeleton file tests/unit/test_grid.py that:",
              "Creates a small 5x5 grid, sets some blocked cells, asserts is_walkable and neighbors results",
              "Tests diagonal True/False produces expected neighbor counts",
              "Acceptance check: Run python -c \"from src.pathfinding.grid import Grid; g=Grid(3,3); assert g.is_walkable(0,0) is True\"",
              "Commit: git add src/pathfinding/grid.py tests/unit/test_grid.py && git commit -m \"feat: add grid map representation for pathfinding\""
            ],
            "done": false
          },
          {
            "number": "6.3",
            "description": "Implement A* pathfinding algorithm in src/pathfinding/a_star.py",
            "details": [
              "Create function a_star(start: Tuple[int,int], goal: Tuple[int,int], grid: Grid, heuristic: str = 'manhattan') -> Optional[List[Tuple[int,int]]]",
              "Use open/closed sets, g-score, f-score, came_from maps",
              "Support heuristics: manhattan, euclidean",
              "Support optional max_nodes_expanded parameter to avoid infinite loops",
              "Return full path list from start to goal inclusive, or None if no path",
              "Implement small helper functions: reconstruct_path(came_from, current)",
              "Add clear error behavior: if start or goal not walkable, return None and log debug message \"a_star: start/goal not walkable\"",
              "Add unit tests tests/unit/test_a_star.py:",
              "Test path exists in empty grid: start=(0,0), goal=(2,2) return length > 0",
              "Test blocked path returns None",
              "Test heuristic choice does not break result",
              "Acceptance check: Run pytest tests/unit/test_a_star.py and assert tests pass locally",
              "Commit: git add src/pathfinding/a_star.py tests/unit/test_a_star.py && git commit -m \"feat: implement A* pathfinding with heuristics\""
            ],
            "done": false
          },
          {
            "number": "6.4",
            "description": "Add optional path smoothing in src/pathfinding/smoothing.py",
            "details": [
              "Implement function smooth_path(path: List[Tuple[int,int]], grid: Grid) -> List[Tuple[int,int]]:",
              "Remove intermediate colinear points by checking line-of-sight using Bresenham or step checking",
              "Ensure resulting path is still walkable; if a removal causes an obstacle, keep original point",
              "Add unit tests tests/unit/test_smoothing.py:",
              "Provide a path with unnecessary waypoints and assert smoothed path is shorter but valid (every segment line-of-sight)",
              "Acceptance check: Run pytest tests/unit/test_smoothing.py and ensure smoothing returns valid path segments",
              "Commit: git add src/pathfinding/smoothing.py tests/unit/test_smoothing.py && git commit -m \"feat: add path smoothing utility\""
            ],
            "done": false
          },
          {
            "number": "6.5",
            "description": "Implement finite state machine (FSM) base and enemy states in src/ai/fsm.py",
            "details": [
              "Create base classes:",
              "class State: on_enter(self, ctx), on_exit(self, ctx), update(self, ctx, dt) \u2014 all methods default to no-op",
              "class FSM: def __init__(self, initial_state: State), def transition(self, new_state: State), def update(self, ctx, dt)",
              "Implement concrete states as classes inheriting State:",
              "PatrolState: holds waypoints list, on_enter picks next waypoint, update moves toward waypoint; emits reached_waypoint transition",
              "ChaseState: receives target entity, update requests pathfinding to follow target; emits lose_target if path fails for X seconds",
              "SearchState: after losing target, move to last known position and search for a short timeout; then transition to patrol",
              "IdleState: stands still",
              "Add clear state API: State may set ctx.next_state_name or call ctx.fsm.transition(...)",
              "Add unit tests tests/unit/test_fsm.py:",
              "Test FSM transition mechanics: initial -> new -> back, and that on_enter/on_exit are called in order",
              "Acceptance check: Run pytest tests/unit/test_fsm.py and ensure transitions occur",
              "Commit: git add src/ai/fsm.py tests/unit/test_fsm.py && git commit -m \"feat: add FSM and core enemy states\""
            ],
            "done": false
          },
          {
            "number": "6.6",
            "description": "Implement EnemyController integrating FSM and pathfinding in src/ai/enemy_controller.py",
            "details": [
              "Create class EnemyController with signature:",
              "def __init__(self, entity: \"EnemyEntity\", grid: Grid, fov: float = 90.0, vision_range: float = 8.0)",
              "Attributes: entity, grid, fsm, target (player or None), current_path (list)",
              "Implement public methods:",
              "set_target(target_entity), clear_target()",
              "update(dt) \u2014 core logic:",
              "Run fsm.update(ctx=self, dt=dt)",
              "If in ChaseState and target set, compute path using a_star when target moves beyond a threshold",
              "If path computed, set entity.waypoints/path and move entity along path",
              "compute_path_to(pos) -> Optional[List[Tuple[int,int]]] uses a_star and smoothing",
              "has_line_of_sight_to(pos) -> bool \u2014 check straight ray for obstacles using grid",
              "Implement logging and explicit error messages:",
              "When compute_path_to fails: log \"EnemyController: no path to target at {pos}\"",
              "Add tests tests/unit/test_enemy_controller.py:",
              "Simulate enemy chasing a moving player in a grid and assert enemy.calculate_path changes appropriately",
              "Test compute_path_to returns None for unreachable target",
              "Acceptance check: Run pytest tests/unit/test_enemy_controller.py; ensure compute_path_to produces expected path or None",
              "Commit: git add src/ai/enemy_controller.py tests/unit/test_enemy_controller.py && git commit -m \"feat: implement EnemyController integrating FSM and pathfinding\""
            ],
            "done": false
          },
          {
            "number": "6.7",
            "description": "Implement Enemy and Player entity classes in src/entities/",
            "details": [
              "Create src/entities/enemy_entity.py with class EnemyEntity:",
              "Attributes: position: Tuple[int,int], speed: float, controller: EnemyController, waypoints: List[Tuple[int,int]]",
              "Methods: update(dt) -> delegates movement to controller; move_toward(target_pos, dt) -> updates position with scalar speed",
              "Provide debug-friendly __repr__ and property to return tile coordinate int(x),int(y)",
              "Create src/entities/player_entity.py with PlayerEntity stub:",
              "Attributes: position, velocity, move_to(x,y) method for test-driven movement",
              "Add unit tests tests/unit/test_entities.py:",
              "Test EnemyEntity.move_toward updates position given speed and dt",
              "Test integration: create grid, enemy, player; set player as target in controller; after update calls enemy moved closer",
              "Acceptance check: Run pytest tests/unit/test_entities.py and confirm movement is correct numeric change",
              "Commit: git add src/entities/enemy_entity.py src/entities/player_entity.py tests/unit/test_entities.py && git commit -m \"feat: add Enemy and Player entity classes\""
            ],
            "done": false
          },
          {
            "number": "6.8",
            "description": "Add debug visualization and logging helpers in src/debug/path_debug.py and src/logging_config.py",
            "details": [
              "Implement src/debug/path_debug.py:",
              "function pretty_print_path(grid: Grid, path: Optional[List[Tuple[int,int]]], size_limit: int = 20) -> str",
              "Prints grid to stdout with characters for walkable '.', blocked '#', start 'S', goal 'G', path '*'",
              "Implement logging config in src/logging_config.py:",
              "configure root logger to output DEBUG level messages to console with timestamps",
              "provide get_logger(name) helper",
              "Update other modules to import get_logger and use debug/info messages where appropriate (a_star, EnemyController)",
              "Add tests tests/unit/test_path_debug.py that asserts pretty_print_path returns a string containing '*' for path coordinates",
              "Acceptance check: Run a small demo python -c \"from src.pathfinding.grid import Grid; from src.pathfinding.a_star import a_star; from src.debug.path_debug import pretty_print_path; g=Grid(5,5); p=a_star((0,0),(4,4),g); print(pretty_print_path(g,p))\" prints a readable map with '*'s",
              "Commit: git add src/debug/path_debug.py src/logging_config.py tests/unit/test_path_debug.py && git commit -m \"feat: add debug path visualization and logging helpers\""
            ],
            "done": false
          },
          {
            "number": "6.9",
            "description": "Write integration demo script src/run_enemy_demo.py and example input map",
            "details": [
              "Create src/run_enemy_demo.py that:",
              "Loads a small hard-coded grid map (5x7) with a few obstacles",
              "Spawns PlayerEntity at (1,1) and EnemyEntity at (5,4)",
              "Sets enemy.controller.set_target(player)",
              "Runs a simple loop for N steps (e.g., 20 iterations) where player moves along a pre-defined route and enemy.update(dt=1.0) is called",
              "Prints debug grid each step using pretty_print_path and shows enemy/player positions and current state",
              "Exposes CLI: python src/run_enemy_demo.py --steps 20",
              "Acceptance check: Run python src/run_enemy_demo.py --steps 10 and observe console output showing enemy path updates and states",
              "Commit: git add src/run_enemy_demo.py && git commit -m \"feat: add demo runner to showcase enemy AI and pathfinding\""
            ],
            "done": false
          },
          {
            "number": "6.10",
            "description": "Add documentation in docs/enemy_ai.md describing usage and public APIs",
            "details": [
              "Create docs/enemy_ai.md covering:",
              "Overview of states (Patrol, Chase, Search, Idle) and how to change behavior (waypoints, vision_range)",
              "Public API signatures: Grid, a_star, smooth_path, EnemyController, EnemyEntity, PlayerEntity",
              "Example usage snippet showing how to spawn an enemy and set a target (copy from run_enemy_demo.py)",
              "Troubleshooting: common error messages e.g., \"a_star: start/goal not walkable\" and recommended fixes",
              "Acceptance check: Open docs/enemy_ai.md and verify it contains at least one code example and the error message section",
              "Commit: git add docs/enemy_ai.md && git commit -m \"docs: document enemy AI, FSM, and pathfinding usage\""
            ],
            "done": false
          },
          {
            "number": "6.11",
            "description": "Add tests covering integration and edge cases in tests/unit and tests/integration",
            "details": [
              "Create tests/integration/test_enemy_chase_integration.py:",
              "Simulate a map where enemy must path around obstacles to reach a moving player",
              "Assert that after N updates enemy reaches a tile adjacent to player or that a path was attempted (current_path not None)",
              "Create tests/unit/test_no_path_and_error_message.py:",
              "Force a blocked goal and assert compute_path_to returns None and that a predictable log entry \"no path to target\" is emitted (use caplog in pytest)",
              "Acceptance check: Run pytest tests/unit tests/integration and ensure all tests pass",
              "Commit: git add tests/integration/test_enemy_chase_integration.py tests/unit/test_no_path_and_error_message.py && git commit -m \"test: add integration tests for enemy chase and no-path logging\""
            ],
            "done": false
          },
          {
            "number": "6.12",
            "description": "Run code quality tools and fix issues",
            "details": [
              "Formatting:",
              "Run: black src/ tests/",
              "Acceptance: No formatting errors remain, code formatted",
              "Linting:",
              "Run: flake8 src/ --max-line-length=88",
              "Acceptance: Fix all reported issues (undefined names, unused imports, long lines). Re-run until clean.",
              "Tests:",
              "Run: pytest -q",
              "Acceptance: All tests pass; if any fail, fix code and re-run",
              "Commit formatting/lint fixes:",
              "git add . && git commit -m \"chore: format code and fix linter warnings\""
            ],
            "done": false
          },
          {
            "number": "6.13",
            "description": "Add user-facing help and error messages for the demo CLI and modules",
            "details": [
              "Update src/run_enemy_demo.py to include argparse and --help text:",
              "Provide flags: --steps, --verbose, --map-file (optional)",
              "On error (e.g., start not walkable), print friendly message and exit non-zero:",
              "Example: print(\"Error: enemy cannot be placed on blocked tile at (x,y). Please choose a valid spawn point.\") and sys.exit(2)",
              "Update docs/enemy_ai.md to document CLI flags and expected messages",
              "Add tests tests/unit/test_demo_cli.py:",
              "Run demo script with --help and assert exit code 0 and help text contains expected flags",
              "Simulate bad map-file input and assert script exits with non-zero and prints the friendly error message",
              "Acceptance check: python src/run_enemy_demo.py --help prints usage; bad map causes the friendly error and non-zero exit",
              "Commit: git add src/run_enemy_demo.py docs/enemy_ai.md tests/unit/test_demo_cli.py && git commit -m \"feat: add CLI help and user-friendly error messages for demo\""
            ],
            "done": false
          },
          {
            "number": "6.14",
            "description": "Final integration: connect EnemyController API to project main entrypoint (src/main.py or update existing main)",
            "details": [
              "If project has a main game loop file, modify src/main.py to:",
              "Import EnemyController, Grid, EnemyEntity, PlayerEntity",
              "Provide an optional demo mode that uses run_enemy_demo logic when --demo flag passed",
              "Ensure code uses logging_config.get_logger to log high-level events",
              "If no main exists, create src/main.py with demo entrypoint and module-level __main__ support:",
              "if __name__ == \"__main__\": parse args and run demo or tests",
              "Acceptance check: Running python -m src.main --demo replicates run_enemy_demo behavior",
              "Commit: git add src/main.py && git commit -m \"feat: integrate enemy AI demo into main entrypoint\""
            ],
            "done": false
          },
          {
            "number": "6.15",
            "description": "Final commit and checklist",
            "details": [
              "Run a final full test and quality sweep:",
              "black src/ tests/",
              "flake8 src/",
              "pytest -q",
              "If all checks pass, commit any remaining changes:",
              "git add . && git commit -m \"chore: finalize enemy AI & pathfinding phase, tests and docs\"",
              "Acceptance check: All tests green, flake8 clean, demo runnable per docs/enemy_ai.md."
            ],
            "done": false
          }
        ]
      },
      {
        "number": 7,
        "title": "Persistence, Saves & Configuration Management",
        "description": null,
        "steps": [
          {
            "number": "7.1",
            "description": "Create persistence & configuration directories and placeholder files",
            "details": [
              "Run: mkdir -p src/config src/persistence src/persistence/migrations src/models tests/unit docs config",
              "Create empty placeholder files:",
              "touch src/config/__init__.py src/config/config.py",
              "touch src/persistence/__init__.py src/persistence/save_manager.py src/persistence/backup.py src/persistence/migrations/README.md",
              "touch src/models/__init__.py src/models/save_state.py",
              "touch tests/unit/test_config.py tests/unit/test_save_manager.py tests/unit/test_backup.py",
              "touch config/config.yaml docs/configuration.md docs/persistence.md",
              "Add a basic .gitignore entry for local DBs and config secrets:",
              "Append to .gitignore: /data/*.db, /config/*.local.yaml, /data/backups/",
              "Acceptance checks:",
              "Files and directories exist (ls shows the created paths)",
              ".gitignore contains the new entries"
            ],
            "done": false
          },
          {
            "number": "7.2",
            "description": "Create default configuration file `config/config.yaml` and schema `config/config.schema.yaml`",
            "details": [
              "Create `config/config.yaml` with keys and default values:",
              "data_dir: \"./data\"",
              "db_filename: \"token_muncher.db\"",
              "autosave_interval_seconds: 60",
              "max_backups: 10",
              "log_level: \"INFO\"",
              "Create `config/config.schema.yaml` describing expected types and constraints:",
              "data_dir: string",
              "db_filename: string",
              "autosave_interval_seconds: integer (>= 0)",
              "max_backups: integer (>= 0)",
              "log_level: enum [DEBUG, INFO, WARNING, ERROR]",
              "Add an example local override `config/config.local.yaml.example` showing how to set env-specific overrides",
              "Acceptance checks:",
              "File `config/config.yaml` loads as valid YAML (run: python -c \"import yaml,sys; yaml.safe_load(open('config/config.yaml'))\")",
              "Schema file exists and contains the described keys"
            ],
            "done": false
          },
          {
            "number": "7.3",
            "description": "Implement configuration loader `src/config/config.py`",
            "details": [
              "Implement a `Config` dataclass with fields matching the schema and methods:",
              "from_file(path: str) -> Config: load YAML file and return instance",
              "from_env() -> dict: read overrides from environment variables TOKEN_MUNCHER_DB_FILENAME, TOKEN_MUNCHER_DATA_DIR, TOKEN_MUNCHER_LOG_LEVEL, etc.",
              "merge(overrides: dict) -> Config: apply overrides on top of file values and validate types/constraints",
              "Use safe YAML loader: add import yaml and dataclasses; do not require external frameworks",
              "Provide validation: raise ValueError with clear message if types or constraints fail (e.g., negative autosave interval)",
              "Add docstring describing environment variables supported and default fallback behavior",
              "Add unit tests in `tests/unit/test_config.py`:",
              "Test loading default config file",
              "Test env var override behavior using os.environ patch",
              "Test validation errors (e.g., set negative interval \u2192 expect ValueError)",
              "Acceptance checks:",
              "Run: python -c \"from src.config.config import Config; print(Config.from_file('config/config.yaml'))\" prints a Config instance",
              "pytest runs tests for config and they pass locally (see later step for running pytest)"
            ],
            "done": false
          },
          {
            "number": "7.4",
            "description": "Commit config work",
            "details": [
              "Commit created files related to configuration:",
              "git add config/config.yaml config/config.schema.yaml config/config.local.yaml.example src/config/config.py tests/unit/test_config.py",
              "git commit -m \"feat: add configuration loader and default YAML config\""
            ],
            "done": false
          },
          {
            "number": "7.5",
            "description": "Define SQLite schema `src/persistence/migrations/001_initial_schema.sql`",
            "details": [
              "Create `src/persistence/migrations/001_initial_schema.sql` containing:",
              "CREATE TABLE saves (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, created_at TEXT NOT NULL, state_json TEXT NOT NULL, metadata_json TEXT);",
              "CREATE TABLE migrations (name TEXT PRIMARY KEY, applied_at TEXT NOT NULL);",
              "CREATE INDEX idx_saves_name ON saves(name);",
              "Create `src/persistence/migrations/README.md` explaining to add numbered migration files and the apply-migrations mechanism",
              "Acceptance checks:",
              "File exists and contains the SQL statements",
              "Basic SQL syntax check: sqlite3 :memory: < src/persistence/migrations/001_initial_schema.sql should not error (if sqlite3 CLI installed)"
            ],
            "done": false
          },
          {
            "number": "7.6",
            "description": "Implement SaveState model `src/models/save_state.py`",
            "details": [
              "Implement a dataclass SaveState with fields:",
              "id: Optional[int] = None",
              "name: str",
              "created_at: str (ISO 8601)",
              "state: dict",
              "metadata: Optional[dict] = None",
              "Add methods:",
              "to_dict(self) -> dict: returns JSON-serializable dict",
              "to_json(self) -> str: returns serialized JSON string",
              "@staticmethod from_row(row: sqlite3.Row) -> SaveState: construct from DB row",
              "@staticmethod from_dict(d: dict) -> SaveState: construct from dict",
              "Add unit tests in tests/unit/test_save_state.py:",
              "Test serialization round-trip: SaveState -> JSON -> SaveState",
              "Test from_row with a simulated sqlite3.Row (use sqlite3.Row or dict)",
              "Acceptance checks:",
              "Running the test file with pytest succeeds for model tests"
            ],
            "done": false
          },
          {
            "number": "7.7",
            "description": "Implement SaveManager `src/persistence/save_manager.py`",
            "details": [
              "Implement `SaveManager` class with methods:",
              "__init__(self, db_path: str): create data directory if missing, open sqlite3 connection with row_factory=sqlite3.Row",
              "init_db(self): ensure migrations applied and run SQL from `src/persistence/migrations/*.sql` that are not yet applied (use migrations table)",
              "save(self, save_state: SaveState) -> int: insert or replace (if id present) and return save id",
              "load(self, save_id: int) -> SaveState: fetch by id and return SaveState or raise KeyError with user-friendly message",
              "list(self, limit: int = 100) -> List[SaveState]: return recent saves ordered by created_at desc",
              "delete(self, save_id: int) -> None: delete row; raise KeyError if not found",
              "close(self): close DB connection",
              "Implement transaction handling and simple retry on sqlite3.OperationalError (locked) with 200ms sleep, 5 retries",
              "Add logging statements (import logging) and clear error messages for user-facing errors",
              "Add unit tests in `tests/unit/test_save_manager.py`:",
              "Test init_db creates tables",
              "Test save -> load -> list behavior",
              "Test delete removes entry and subsequent load raises KeyError",
              "Acceptance checks:",
              "Run a small script: python -c \"from src.persistence.save_manager import SaveManager; sm=SaveManager('data/test.db'); sm.init_db(); print('ok')\" prints ok and data/test.db created",
              "pytest unit tests for save_manager pass"
            ],
            "done": false
          },
          {
            "number": "7.8",
            "description": "Commit persistence core implementation",
            "details": [
              "Commit the migration and SaveManager code and tests:",
              "git add src/persistence src/models tests/unit/test_save_manager.py tests/unit/test_save_state.py src/persistence/migrations/001_initial_schema.sql",
              "git commit -m \"feat: implement SQLite persistence manager and initial schema + SaveState model\""
            ],
            "done": false
          },
          {
            "number": "7.9",
            "description": "Implement backup/export/import in `src/persistence/backup.py`",
            "details": [
              "Implement functions:",
              "export_save_to_json(save_id: int, db_path: str, out_path: str) -> str: load save by id and write pretty JSON to out_path (create directories as needed)",
              "import_save_from_json(json_path: str, db_path: str) -> int: read JSON and insert as new save (update created_at if present)",
              "maintain a backups directory under configured data_dir and implement rotating backup (respect max_backups config) when export is called",
              "Add explicit error messages for file-not-found, invalid JSON, and DB errors",
              "Add unit tests in `tests/unit/test_backup.py`:",
              "Round-trip export/import: create save, export to file, import to new DB, then verify content equivalence",
              "Test rotation: create > max_backups exports, verify older files removed",
              "Acceptance checks:",
              "Running a small script performs export and import successfully and persisted entries appear in DB",
              "pytest backup tests pass"
            ],
            "done": false
          },
          {
            "number": "7.10",
            "description": "Integrate persistence and config into CLI `src/cli.py`",
            "details": [
              "Implement an argparse-based CLI with subcommands:",
              "init: initialize DB and create data_dir (usage: token-muncher init)",
              "save --name NAME --file state.json: read JSON state file and save",
              "load --id ID --out output.json: write the save state to output file",
              "list [--limit N]: print table of saves (id, name, created_at)",
              "export --id ID --out path.json: call export_save_to_json",
              "import --file path.json: call import_save_from_json",
              "config --print: print effective configuration (merged file+env)",
              "On startup, CLI should:",
              "Load config via Config.from_file('config/config.yaml') and apply env overrides",
              "Initialize SaveManager with db_path = os.path.join(config.data_dir, config.db_filename)",
              "Provide helpful help text for each subcommand and clear error messages for missing files/invalid IDs",
              "Add unit tests for CLI parsing in tests/unit/test_cli.py using subprocess or argparse parse_args invocation",
              "Acceptance checks:",
              "Run: python src/cli.py --help shows subcommands and descriptions",
              "Example flow (init -> save -> list -> load) runs without unhandled exceptions"
            ],
            "done": false
          },
          {
            "number": "7.11",
            "description": "Add migrations runner in SaveManager and a sample migration `src/persistence/migrations/002_add_description.sql`",
            "details": [
              "Implement `apply_migrations()` in SaveManager:",
              "Read migration files in numeric order (001_*.sql, 002_*.sql)",
              "For each not present in migrations table, run SQL in a transaction and insert row into migrations(name, applied_at)",
              "Ensure idempotency: if a migration producer failed halfway, subsequent runs will detect or raise with clear message",
              "Create `src/persistence/migrations/002_add_description.sql`:",
              "ALTER TABLE saves ADD COLUMN description TEXT; and include an IF NOT EXISTS style wrapper for sqlite (use CREATE TABLE temp then copy pattern or a safe check)",
              "Add unit tests that:",
              "Simulate adding the second migration and ensure apply_migrations applies only new migrations",
              "Acceptance checks:",
              "Initialize DB on a fresh path and verify migrations table contains entries for applied migrations"
            ],
            "done": false
          },
          {
            "number": "7.12",
            "description": "Commit CLI and migrations improvements",
            "details": [
              "Commit CLI, migrations runner, and backup code:",
              "git add src/cli.py src/persistence/backup.py src/persistence/migrations/002_add_description.sql src/persistence/save_manager.py",
              "git commit -m \"feat: add CLI commands, migrations runner and backup/export/import support\""
            ],
            "done": false
          },
          {
            "number": "7.13",
            "description": "Write comprehensive unit and integration tests",
            "details": [
              "Create integration tests at `tests/integration/test_save_load_flow.py` that perform full scenario:",
              "Use a temporary directory (pytest tmp_path) to create config override and DB",
              "Run CLI init, save (from small JSON file), list, export, import and load; assert outputs and DB contents match expected",
              "Expand unit tests where missing:",
              "tests/unit/test_backup.py and test_save_manager.py should include concurrency/simulated locked DB tests (use threads if necessary)",
              "tests/unit/test_config.py should include edge cases for missing config file (fallback to defaults)",
              "Add test utilities: `tests/conftest.py` with fixtures for temp config and in-memory DB usage",
              "Acceptance checks:",
              "Run: pytest tests/unit tests/integration and all tests pass",
              "Integration test executes CLI subcommands successfully"
            ],
            "done": false
          },
          {
            "number": "7.14",
            "description": "Run linters and formatters, fix style issues",
            "details": [
              "Run black and isort:",
              "black src/ tests/",
              "isort src/",
              "Run flake8:",
              "flake8 src/ --max-line-length=120",
              "Fix any reported errors (unused imports, missing docstrings are warnings \u2014 address critical issues)",
              "Run mypy (optional if type hints added):",
              "mypy src/ --ignore-missing-imports",
              "Acceptance checks:",
              "black reports no changes on a second run",
              "flake8 exits with code 0 (no blocking errors)"
            ],
            "done": false
          },
          {
            "number": "7.15",
            "description": "Commit tests and code quality fixes",
            "details": [
              "Commit all test files and any linting/formatting changes:",
              "git add src/ tests/ pyproject.toml .flake8",
              "git commit -m \"test: add unit + integration tests and chore: apply code formatting\""
            ],
            "done": false
          },
          {
            "number": "7.16",
            "description": "Document persistence and configuration in `docs/`",
            "details": [
              "Update `docs/configuration.md` with:",
              "Explanation of config keys in `config/config.yaml`",
              "How to override via environment variables and a local config file",
              "Example: export TOKEN_MUNCHER_DB_FILENAME=my.db",
              "Update `docs/persistence.md` with:",
              "How to initialize the DB, run migrations, backup and restore commands",
              "Example CLI usage for save, load, export, import with sample commands and expected outputs",
              "Error messages and troubleshooting steps (e.g., DB locked -> retry)",
              "Add short example snippets (copy-paste ready) for CLI usage and config files",
              "Acceptance checks:",
              "Files docs/configuration.md and docs/persistence.md exist and contain the examples",
              "A reviewer can follow docs to run the example commands locally"
            ],
            "done": false
          },
          {
            "number": "7.17",
            "description": "Add user-facing help text and examples into README or CLI help and commit",
            "details": [
              "Update README.md or top-level docs with:",
              "Quickstart section: install, run `token-muncher init`, create a save, list, load",
              "Example commands with expected outputs",
              "Ensure CLI subcommands include concise usage and examples via argparse epilog or --help",
              "Commit documentation changes:",
              "git add README.md docs/configuration.md docs/persistence.md",
              "git commit -m \"docs: add configuration and persistence usage + CLI examples\""
            ],
            "done": false
          },
          {
            "number": "7.18",
            "description": "Final verification run and commit",
            "details": [
              "Run the full verification script locally:",
              "black src/ tests/ && flake8 src/ && pytest -q",
              "If all pass, commit any final small fixes:",
              "git add -A",
              "git commit -m \"chore: finalize persistence, config management and tests\"",
              "Acceptance checks:",
              "black exits clean, flake8 returns 0, pytest all tests pass",
              "Project can perform: python src/cli.py init && python src/cli.py save --name \"test\" --file samples/state.json && python src/cli.py list and get expected results"
            ],
            "done": false
          },
          {
            "number": "7.19",
            "description": "Add CI job notes (optional CI config file) to run checks automatically",
            "details": [
              "Create `.github/workflows/ci.yml` or `ci/.gitlab-ci.yml` with steps:",
              "Check out code, set up Python, install dependencies (PyYAML, pytest, flake8, black), run black --check, flake8, pytest",
              "Add the file and commit:",
              "git add .github/workflows/ci.yml",
              "git commit -m \"chore(ci): add CI workflow to run formatting, linting and tests\"",
              "Acceptance checks:",
              "CI config file exists and contains steps to run black --check, flake8 and pytest"
            ],
            "done": false
          },
          {
            "number": "7.20",
            "description": "Provide upgrade & backup maintenance commands (maintenance helpers)",
            "details": [
              "Add utility functions in `src/persistence/maintenance.py`:",
              "prune_backups(data_dir: str, keep: int): remove oldest backups keeping latest N",
              "compact_db(db_path: str): run VACUUM and optionally run PRAGMA optimize",
              "export_all_saves(db_path: str, out_dir: str): write all saves to individual JSON files",
              "Hook these utilities into CLI under subcommands `maintenance prune`, `maintenance compact`, `maintenance export-all`",
              "Add tests for maintenance utilities in tests/unit/test_maintenance.py",
              "Acceptance checks:",
              "Running maintenance commands produces expected files and DB compaction runs without error",
              "Tests for maintenance pass"
            ],
            "done": false
          }
        ]
      },
      {
        "number": 8,
        "title": "Testing, QA & CI Maturity",
        "description": null,
        "steps": [
          {
            "number": "8.1",
            "description": "Create test infrastructure files and development dependencies list",
            "details": [
              "Create project dev dependency file `requirements-dev.txt` with at least: pytest, pytest-cov, pytest-rerunfailures, black, flake8, isort, pre-commit, mypy, coverage. Example content:",
              "Open/create `requirements-dev.txt` and add:",
              "pytest>=7.0",
              "pytest-cov",
              "pytest-rerunfailures",
              "black",
              "flake8",
              "isort",
              "pre-commit",
              "mypy",
              "coverage",
              "Create a pytest configuration file `pytest.ini` in project root with sensible defaults:",
              "Add content (example):",
              "[pytest]",
              "minversion = 6.0",
              "testpaths = tests",
              "addopts = --strict-markers -q",
              "markers =",
              "integration: integration tests",
              "Create coverage config file `.coveragerc` in project root with basic exclusions:",
              "Add content (example):",
              "[run]",
              "branch = True",
              "omit =",
              "tests/*",
              ".venv/*",
              "docs/*",
              "Acceptance checks:",
              "Run: python -m pip install -r requirements-dev.txt",
              "Run: pytest -q (should run 0 tests without crashing pytest; exit code 0 or show \"collected 0 items\")",
              "Commit:",
              "git add requirements-dev.txt pytest.ini .coveragerc && git commit -m \"chore: add test infra and dev dependencies\""
            ],
            "done": false
          },
          {
            "number": "8.2",
            "description": "Add a minimal, testable core module to `src/token_muncher` if missing",
            "details": [
              "Create package directory and init: `mkdir -p src/token_muncher && touch src/token_muncher/__init__.py`",
              "Create `src/token_muncher/core.py` with a simple class and function to exercise tests:",
              "Implement `class TokenMuncher` with method `munch(tokens: list[str]) -> dict` that returns {\"count\": int, \"tokens\": list}.",
              "Implement a top-level function `munch_tokens(tokens: list[str]) -> int` that returns the count and raises TypeError when input is not a list.",
              "Example behavior: munch([]) -> {\"count\": 0, \"tokens\": []}; munch_tokens([\"a\",\"b\"]) -> 2",
              "Add docstrings and simple type hints in `src/token_muncher/core.py`.",
              "Acceptance checks:",
              "Run in Python REPL: python -c \"from token_muncher.core import munch_tokens; print(munch_tokens(['x']))\" and verify output is \"1\"",
              "Commit:",
              "git add src/token_muncher/__init__.py src/token_muncher/core.py && git commit -m \"feat: add TokenMuncher core implementation for testing\""
            ],
            "done": false
          },
          {
            "number": "8.3",
            "description": "Write unit tests for the core functionality",
            "details": [
              "Create tests directory and unit test file `tests/unit/test_core.py`",
              "Test cases to include:",
              "test_munch_tokens_counts_correctly: assert munch_tokens([\"a\",\"b\"]) == 2",
              "test_munch_tokens_empty_list: assert munch_tokens([]) == 0",
              "test_munch_tokens_type_error: with pytest.raises(TypeError) when passing None or string",
              "test_TokenMuncher_returns_structure: use TokenMuncher().munch(['a']) returns dict with keys 'count' and 'tokens'",
              "Use pytest parametrization to cover multiple inputs (at least 3 different inputs).",
              "Run tests locally: pytest tests/unit -q",
              "Acceptance checks:",
              "All tests in `tests/unit/test_core.py` pass (exit code 0).",
              "Commit:",
              "git add tests/unit/test_core.py && git commit -m \"test: add unit tests for TokenMuncher core\""
            ],
            "done": false
          },
          {
            "number": "8.4",
            "description": "Add integration tests that exercise higher-level flows (file/CLI simulated)",
            "details": [
              "Create `tests/integration/test_cli_flow.py`",
              "Create an integration test that imports `TokenMuncher` and simulates a simple file input:",
              "Use `tmp_path` fixture to write `input.json` with tokens `[\"a\",\"b\",\"c\"]`.",
              "Call a helper function to load file and call `TokenMuncher.munch`.",
              "Assert output structure and that count == 3.",
              "If CLI exists or will be created, also include a subprocess test calling `python -m token_muncher.cli --input <path>` and validate stdout contains count.",
              "If CLI module doesn't exist yet, create a minimal CLI wrapper `src/token_muncher/cli.py`:",
              "Implement `def main(argv=None)` that reads JSON file path from `--input` and prints JSON {\"count\":N}.",
              "Add `if __name__ == \"__main__\": main()` at bottom.",
              "Run integration tests: pytest tests/integration -q",
              "Acceptance checks:",
              "Integration tests pass and return exit code 0",
              "Commit:",
              "git add tests/integration/test_cli_flow.py src/token_muncher/cli.py && git commit -m \"test: add integration tests and minimal CLI wrapper\""
            ],
            "done": false
          },
          {
            "number": "8.5",
            "description": "Add linters and formatting configuration files",
            "details": [
              "Create/modify `pyproject.toml` in project root with black and isort settings and project metadata; include:",
              "[tool.black] line-length = 88",
              "[tool.isort] profile = \"black\"",
              "Create `.flake8` config file with content:",
              "[flake8]",
              "max-line-length = 88",
              "exclude = .git,__pycache__,docs,build,dist",
              "Create `.pre-commit-config.yaml` with hooks for black, isort, flake8, and trailing-whitespace removal. Example hooks:",
              "repos: https://github.com/psf/black \u2014 hook: black",
              "repos: https://github.com/pre-commit/mirrors-isort \u2014 hook: isort",
              "repos: https://gitlab.com/pycqa/flake8 \u2014 hook: flake8",
              "Install pre-commit hooks locally:",
              "Run: pre-commit install",
              "Run once on entire repo: pre-commit run --all-files",
              "Run formatters and linters manually:",
              "Run: black src/ tests/",
              "Run: isort src/ tests/",
              "Run: flake8 src/ tests/",
              "Acceptance checks:",
              "black and isort complete without errors and flake8 exits with code 0 (or shows fixable warnings if you choose to fix them)",
              "Commit:",
              "git add pyproject.toml .flake8 .pre-commit-config.yaml && git commit -m \"chore: add formatting and linting configuration\""
            ],
            "done": false
          },
          {
            "number": "8.6",
            "description": "Add static type checking (mypy) configuration and initial fixes",
            "details": [
              "Create `mypy.ini` or add to `pyproject.toml` a mypy section; create file `mypy.ini` in project root:",
              "[mypy]",
              "python_version = 3.9",
              "ignore_missing_imports = True",
              "disallow_untyped_defs = False",
              "Run mypy and fix simple typing issues in the code:",
              "Run: mypy src/",
              "Add type hints to `src/token_muncher/core.py` (function signatures and return types) and typings in tests if needed.",
              "Acceptance checks:",
              "mypy src/ completes without fatal errors (exit code 0)",
              "Commit:",
              "git add mypy.ini src/token_muncher/core.py && git commit -m \"chore: add mypy config and basic typing for core module\""
            ],
            "done": false
          },
          {
            "number": "8.7",
            "description": "Add CI workflow for GitHub Actions with lint, test, coverage and caching",
            "details": [
              "Create workflow file `.github/workflows/ci.yml`",
              "Define matrix job for python: [3.9, 3.10, 3.11]",
              "Steps:",
              "checkout",
              "cache pip (use actions/cache with key: pip-${{ matrix.python-version }}-requirements)",
              "setup-python",
              "install dev dependencies: python -m pip install -r requirements-dev.txt",
              "run pre-commit hooks: pre-commit run --all-files",
              "run lint: black --check src/ tests/ ; isort --check-only src/ tests/ ; flake8 src/ tests/",
              "run tests with coverage: pytest --cov=src --cov-report=xml",
              "upload coverage artifact and optional Codecov upload (add conditional)",
              "Ensure the workflow fails if lint or tests fail.",
              "Acceptance checks:",
              "Locally simulate CI steps:",
              "python -m pip install -r requirements-dev.txt",
              "pre-commit run --all-files",
              "black --check src/ tests/ (should be OK)",
              "pytest --cov=src --cov-report=term-missing",
              "Commit:",
              "git add .github/workflows/ci.yml && git commit -m \"ci: add GitHub Actions workflow for linting, testing and coverage\""
            ],
            "done": false
          },
          {
            "number": "8.8",
            "description": "Enforce coverage threshold and add coverage reporting",
            "details": [
              "Update `pytest.ini` add `addopts = --strict-markers -q --cov=src --cov-report=term-missing`",
              "Create a small CI step in `.github/workflows/ci.yml` to fail if coverage < 80%:",
              "After pytest, add a step running `bash -lc \"coverage report --fail-under=80\"`",
              "Optionally add Codecov integration: add a job step to upload `coverage.xml` to Codecov using `codecov` action (only if CODECOV_TOKEN or public repo).",
              "Acceptance checks:",
              "Run locally: pytest --cov=src && coverage report --fail-under=80 (will pass if coverage >=80, else fail)",
              "Commit:",
              "git add pytest.ini .github/workflows/ci.yml && git commit -m \"chore: enforce test coverage threshold and add coverage reporting\""
            ],
            "done": false
          },
          {
            "number": "8.9",
            "description": "Add retry/backoff for flaky tests and isolate integration tests",
            "details": [
              "Update `pytest.ini` to include reruns for flaky tests:",
              "addopts should include `--reruns 2 --maxfail=1` or similar for flaky handling",
              "Mark integration tests with `@pytest.mark.integration` and configure CI to run integration tests separately:",
              "Modify `.github/workflows/ci.yml` to run unit tests first: `pytest -m \"not integration\"`",
              "Add a second job (or step) to run integration tests (may be slower) and separate failure handling.",
              "Acceptance checks:",
              "Run: pytest -q (unit tests pass) and pytest -q -m integration (integration tests run)",
              "Commit:",
              "git add pytest.ini tests/integration/test_cli_flow.py .github/workflows/ci.yml && git commit -m \"test: add flaky test rerun policy and separate integration tests\""
            ],
            "done": false
          },
          {
            "number": "8.10",
            "description": "Create test fixtures and sample test data files",
            "details": [
              "Add a fixtures directory `tests/fixtures/` and add sample JSON `tests/fixtures/sample_tokens.json` with content: [\"alpha\",\"beta\",\"gamma\"]",
              "Add a fixture file `tests/conftest.py` exporting commonly used fixtures:",
              "`json_file` fixture that writes given data to `tmp_path` and returns the path",
              "`sample_tokens` fixture that returns the list `[\"alpha\",\"beta\",\"gamma\"]`",
              "Update unit/integration tests to use these fixtures instead of duplicating data.",
              "Acceptance checks:",
              "Run pytest -q and confirm tests referencing fixtures pass",
              "Commit:",
              "git add tests/fixtures/sample_tokens.json tests/conftest.py && git commit -m \"test: add fixtures and sample data for unit and integration tests\""
            ],
            "done": false
          },
          {
            "number": "8.11",
            "description": "Add a smoke-test script and a QA script for local CI preflight",
            "details": [
              "Create `scripts/smoke_test.sh` (executable) in repo root:",
              "Script runs: black --check, flake8, mypy, pytest -q --maxfail=1",
              "Exit with non-zero if any command fails",
              "Create `scripts/ci_preflight.sh` that mirrors CI steps (install deps into venv, run pre-commit, run unit tests, run coverage check)",
              "Make both scripts executable: chmod +x scripts/smoke_test.sh scripts/ci_preflight.sh",
              "Acceptance checks:",
              "Run: ./scripts/smoke_test.sh and verify it completes with exit code 0 on a clean repo",
              "Commit:",
              "git add scripts/smoke_test.sh scripts/ci_preflight.sh && git commit -m \"chore: add smoke test and CI preflight scripts\""
            ],
            "done": false
          },
          {
            "number": "8.12",
            "description": "Add QA documentation and user-facing help text",
            "details": [
              "Create `docs/testing.md` documenting:",
              "How to run unit tests: `pytest tests/unit`",
              "How to run all tests and coverage: `pytest --cov=src`",
              "How to run linters and formatters: `black src/ tests/`, `flake8 src/ tests/`",
              "How to run smoke tests: `./scripts/smoke_test.sh`",
              "How to interpret CI results on GitHub Actions",
              "Update CLI help string in `src/token_muncher/cli.py` to include usage examples and error messages:",
              "Implement `--help` text, validate input path and print friendly error messages (exit code 2) if file missing or invalid JSON.",
              "Acceptance checks:",
              "Open `docs/testing.md` and confirm commands are copy/paste runnable",
              "Run `python -m token_muncher.cli --help` and verify help displays usage and examples",
              "Commit:",
              "git add docs/testing.md src/token_muncher/cli.py && git commit -m \"docs: add QA testing docs and improve CLI help text\""
            ],
            "done": false
          },
          {
            "number": "8.13",
            "description": "Harden CI: caching, artifacts, and badges",
            "details": [
              "Update `.github/workflows/ci.yml` to:",
              "Cache pip wheel files using `actions/cache` keyed by `requirements-dev.txt` hash",
              "Upload test results and coverage xml as artifacts using `actions/upload-artifact`",
              "Add job outputs and create a repository README badge snippet for build status and coverage (use shields.io links)",
              "Add README snippet: update `README.md` with \"CI status\" and \"coverage\" badges linking to GitHub Actions and Codecov (or placeholder)",
              "Acceptance checks:",
              "Commit CI workflow and run (or rely on GitHub to run) and confirm workflow shows cache usage and artifacts appear on job run",
              "Commit:",
              "git add .github/workflows/ci.yml README.md && git commit -m \"chore(ci): add caching, artifacts and CI badges\""
            ],
            "done": false
          },
          {
            "number": "8.14",
            "description": "Set up branch protection and CI gating (documented)",
            "details": [
              "Add `docs/ci_policies.md` describing required branch protection rules to be applied in repository settings:",
              "Require status checks to pass: `ci` workflow job names for lint and test",
              "Require code owner reviews (optional)",
              "Enforce \"Require branches to be up to date before merging\"",
              "Provide exact GitHub UI steps in the doc so a human can enable protections (cannot modify repo settings from code).",
              "Acceptance checks:",
              "Confirm `docs/ci_policies.md` contains step-by-step instructions to configure branch protection with exact job names used in `.github/workflows/ci.yml`",
              "Commit:",
              "git add docs/ci_policies.md && git commit -m \"docs: document branch protection and CI gating policy\""
            ],
            "done": false
          },
          {
            "number": "8.15",
            "description": "Final QA run, fix issues, and final commit milestone",
            "details": [
              "Run the full QA pipeline locally:",
              "python -m pip install -r requirements-dev.txt",
              "./scripts/ci_preflight.sh",
              "./scripts/smoke_test.sh",
              "pytest --cov=src --cov-report=term-missing",
              "Fix any issues reported by linters, mypy, or failing tests. For each logical fix, create focused commits:",
              "Example commit message patterns:",
              "fix: correct type hint in src/token_muncher/core.py",
              "fix: handle empty input in cli help and error messages",
              "test: add missing edge-case test for munch_tokens",
              "After all issues are fixed and all checks pass, make a final commit summarizing QA:",
              "git add -A && git commit -m \"chore: finalize testing, QA and CI maturity setup\"",
              "Acceptance checks:",
              "All local commands run without errors",
              "Git history contains the commits added in earlier steps and final commit",
              "No further steps; phase complete."
            ],
            "done": false
          }
        ]
      },
      {
        "number": 9,
        "title": "UX, Accessibility & Documentation",
        "description": null,
        "steps": [
          {
            "number": "9.1",
            "description": "Create docs skeleton and UX overview files",
            "details": [
              "Create directory: docs/",
              "Create file: docs/README.md \u2014 add top-level navigation links to UX, Accessibility, Usage, Screenshots, Changelog",
              "Add headings: \"# Token Muncher Documentation\", \"## UX\", \"## Accessibility\", \"## Usage\", \"## Screenshots\", \"## Changelog\"",
              "Acceptance: file exists and contains the listed headings (run: grep -E \"UX|Accessibility|Usage|Screenshots|Changelog\" docs/README.md)",
              "Create file: docs/UX.md \u2014 describe visual goals, user flows, and personas",
              "Add sections: \"Overview\", \"Primary Flows\", \"Microcopy Style\", \"Interaction Guidelines\"",
              "Acceptance: file contains \"Primary Flows\" and at least one bullet list describing flows",
              "Create file: docs/USAGE.md \u2014 quickstart with example commands/screenshots",
              "Include a \"Quickstart\" code block showing start/build commands and at least one screenshot reference docs/screenshots/quickstart.png",
              "Acceptance: file contains \"Quickstart\" and a fenced code block",
              "Commit docs skeleton:",
              "Run: git add docs/ && git commit -m \"docs: add documentation skeleton (UX, Usage, README)\""
            ],
            "done": false
          },
          {
            "number": "9.2",
            "description": "Add Accessibility documentation and checklist",
            "details": [
              "Create file: docs/ACCESSIBILITY.md",
              "Add sections: \"Goal\", \"Checklist (keyboard, color contrast, ARIA, semantics)\", \"How to run automated checks\", \"How to file accessibility bugs\"",
              "Provide explicit checklist items with expected acceptance criteria (e.g., \"All interactive elements reachable by Tab\", \"Color contrast >= 4.5:1 for normal text\")",
              "Acceptance: file contains the \"Checklist\" header and 6+ checklist items",
              "Add a \"Running tests\" subsection listing the CLI commands we'll use later (axe, pa11y, Lighthouse)",
              "Include example commands: npx jest --runTestsByPath tests/accessibility/test_a11y.js, npx pa11y http://localhost:3000",
              "Commit accessibility docs:",
              "Run: git add docs/ACCESSIBILITY.md && git commit -m \"docs: add accessibility checklist and test commands\""
            ],
            "done": false
          },
          {
            "number": "9.3",
            "description": "Add style and microcopy guide",
            "details": [
              "Create file: docs/STYLE_GUIDE.md",
              "Add sections: \"Color tokens\", \"Spacing scale\", \"Typography\", \"Button styles\", \"Form labels & validation copy\", \"Voice & tone examples\"",
              "Add example microcopy for success, error, empty states (short, actionable lines)",
              "Add color token examples (names + hex) to be synced with code in later steps (e.g., --color-primary: #0b5fff)",
              "Acceptance: file contains at least 6 color tokens, spacing scale of 4 values, and 3 example messages for UX copy",
              "Commit style guide:",
              "Run: git add docs/STYLE_GUIDE.md && git commit -m \"docs: add UI style guide and microcopy rules\""
            ],
            "done": false
          },
          {
            "number": "9.4",
            "description": "Create CONTRIBUTING and templates",
            "details": [
              "Create file: CONTRIBUTING.md with instructions for PRs, branching, tests, and accessibility checks to run before submitting",
              "Include \"Run tests: npm test\" and \"Run accessibility checks: npx pa11y <url> or npx jest tests/accessibility\"",
              "Create file: CODE_OF_CONDUCT.md using a short standard (add a link to the full text or paste a short version)",
              "Create .github/PULL_REQUEST_TEMPLATE.md with checklist items: \"Adds/updates docs\", \"Includes tests\", \"Passes linters\"",
              "Acceptance: files exist and PR template includes checklist",
              "Commit contributing docs:",
              "Run: git add CONTRIBUTING.md CODE_OF_CONDUCT.md .github/PULL_REQUEST_TEMPLATE.md && git commit -m \"docs: add contributing guide and PR template\""
            ],
            "done": false
          },
          {
            "number": "9.5",
            "description": "Implement an accessible Help modal component",
            "details": [
              "Create file: src/components/HelpModal.jsx",
              "Implement a functional component that:",
              "Receives props: isOpen (bool), onClose (func)",
              "Renders a dialog with role=\"dialog\" and aria-modal=\"true\"",
              "Has an h2 title with id=\"help-title\" and aria-labelledby referencing that id",
              "Implements keyboard Close on Esc and Close button with aria-label=\"Close help\"",
              "Traps focus when open (use a simple focus trap utility or set initial focus to close button)",
              "Create file: src/components/HelpModal.css",
              "Add focus-visible styles, visible focus outlines, and responsive rules",
              "Add usage example in src/components/AppHeader.jsx (import and open HelpModal on ? or Help button)",
              "Acceptance: When HelpModal is open, focus lands on the Close button and pressing Esc closes it",
              "Create test: tests/unit/test_helpmodal.test.jsx",
              "Use @testing-library/react to render HelpModal, assert role=\"dialog\", focus behaviour, and Esc handling",
              "Run: npx jest tests/unit/test_helpmodal.test.jsx and expect tests to pass",
              "Commit Help modal:",
              "Run: git add src/components/HelpModal.jsx src/components/HelpModal.css src/components/AppHeader.jsx tests/unit/test_helpmodal.test.jsx && git commit -m \"feat: add accessible Help modal with keyboard support and tests\""
            ],
            "done": false
          },
          {
            "number": "9.6",
            "description": "Add accessible form field component for token input",
            "details": [
              "Create file: src/components/TokenInput.jsx",
              "Implement input with label element (for attribute), aria-describedby linking to a helper text element (id=\"token-help\")",
              "Add inline validation: when invalid show a <p id=\"token-error\" role=\"alert\"> with an accessible error message",
              "Include placeholder text, max length attribute, and aria-invalid when validation fails",
              "Create file: src/components/TokenInput.css \u2014 ensure visible focus and inline helper styling",
              "Create unit tests: tests/unit/test_tokeninput.test.jsx",
              "Tests verify label association, aria-describedby exists, error shows role=\"alert\" when invalid, and aria-invalid toggles correctly",
              "Run: npx jest tests/unit/test_tokeninput.test.jsx",
              "Commit TokenInput:",
              "Run: git add src/components/TokenInput.jsx src/components/TokenInput.css tests/unit/test_tokeninput.test.jsx && git commit -m \"feat: implement accessible TokenInput with inline validation and tests\""
            ],
            "done": false
          },
          {
            "number": "9.7",
            "description": "Implement focus management utilities and integrate into main layout",
            "details": [
              "Create file: src/utils/focusManager.js",
              "Export functions: trapFocus(containerElement), releaseFocus()",
              "Implement basic algorithm: save previously focused element, focus first tabbable inside container, prevent tabbing out (handle Tab/Shift+Tab)",
              "Modify: src/components/AppLayout.jsx",
              "When any modal (HelpModal) opens, call trapFocus(modalElement); when closes, call releaseFocus()",
              "Create tests: tests/unit/test_focusmanager.test.js",
              "Test that trapFocus focuses an element inside container and releaseFocus restores previous focus",
              "Run: npx jest tests/unit/test_focusmanager.test.js",
              "Commit focus manager:",
              "Run: git add src/utils/focusManager.js src/components/AppLayout.jsx tests/unit/test_focusmanager.test.js && git commit -m \"feat: add focusManager utility and integrate with AppLayout\""
            ],
            "done": false
          },
          {
            "number": "9.8",
            "description": "Add ARIA roles and states to shared button and modal components",
            "details": [
              "Update file: src/components/Button.jsx",
              "Ensure props accept aria-pressed, aria-expanded, role if necessary; render as <button>",
              "Add visible focus styles in src/components/Button.css",
              "Update file: src/components/Modal.jsx (if present) or update HelpModal to provide aria-labelledby and aria-describedby properly",
              "Ensure keyboard interactions are implemented: Enter triggers, Space triggers, and disable pointer-only assumptions",
              "Create tests: tests/unit/test_button_aria.test.jsx and tests/unit/test_modal_aria.test.jsx",
              "Validate role attributes, aria-pressed toggling, aria-expanded state changes, and keyboard activation",
              "Run: npx jest tests/unit/test_button_aria.test.jsx tests/unit/test_modal_aria.test.jsx",
              "Commit ARIA updates:",
              "Run: git add src/components/Button.jsx src/components/Button.css src/components/Modal.jsx tests/unit/test_button_aria.test.jsx tests/unit/test_modal_aria.test.jsx && git commit -m \"fix: add ARIA states to Button and Modal components and tests\""
            ],
            "done": false
          },
          {
            "number": "9.9",
            "description": "Implement theme tokens and color contrast verification",
            "details": [
              "Create file: src/styles/theme.css (or src/styles/theme.js if using CSS-in-JS)",
              "Define CSS variables (or JS constants) for color tokens used in STYLE_GUIDE.md, e.g., --color-primary: #0b5fff, --color-on-primary: #ffffff",
              "Update components to consume tokens (e.g., Button.css, HelpModal.css)",
              "Replace hard-coded colors with var(--color-*)",
              "Create small script: scripts/check-contrast.js",
              "Use the 'color' npm package or a simple contrast function to check text colors in theme tokens against accessibility ratios; implement Node script that reads theme file and prints PASS/FAIL",
              "Run: node scripts/check-contrast.js and expect \"PASS\" for token pairs used for body and headings",
              "Acceptance: script outputs PASS for primary token pairs; if FAIL, change color hex in src/styles/theme.css until PASS",
              "Commit theme and script:",
              "Run: git add src/styles/theme.css scripts/check-contrast.js && git commit -m \"feat: add theme tokens and color contrast check script\""
            ],
            "done": false
          },
          {
            "number": "9.10",
            "description": "Add automated accessibility tests using jest-axe and axe-core",
            "details": [
              "Install dev dependencies (run in project root): npm install --save-dev jest @testing-library/react axe-core jest-axe",
              "Create file: tests/accessibility/test_a11y.test.jsx",
              "Render top-level pages/components with @testing-library/react and run axe(document.body) via jest-axe assertions",
              "Include two tests: one for the main app shell, one for the HelpModal component open state",
              "Run tests: npx jest tests/accessibility/test_a11y.test.jsx",
              "Acceptance: test run completes and reports zero accessibility violations (or only documented exceptions with comments)",
              "Commit accessibility tests:",
              "Run: git add package.json package-lock.json tests/accessibility/test_a11y.test.jsx && git commit -m \"test: add automated accessibility tests with jest-axe\""
            ],
            "done": false
          },
          {
            "number": "9.11",
            "description": "Add pa11y and Lighthouse scripts for CI/manual audits",
            "details": [
              "Add npm scripts to package.json:",
              "\"audit:pa11y\": \"pa11y http://localhost:3000 --config pa11y.json --reporter cli\"",
              "\"audit:lighthouse\": \"lighthouse http://localhost:3000 --output html --output-path ./docs/lighthouse-report.html --chrome-flags='--headless'\"",
              "Create pa11y.json config file with standard rules (timeout, include warnings)",
              "Add script file: scripts/run-lighthouse.sh",
              "Content: start dev server (npm run dev &), wait for server, run npm run audit:lighthouse, kill server",
              "Run an audit locally: npm run audit:pa11y and ./scripts/run-lighthouse.sh",
              "Acceptance: pa11y outputs no \"ERROR\" level results and docs/lighthouse-report.html exists",
              "Commit audit scripts and config:",
              "Run: git add package.json pa11y.json scripts/run-lighthouse.sh && git commit -m \"chore: add pa11y and Lighthouse audit scripts\""
            ],
            "done": false
          },
          {
            "number": "9.12",
            "description": "Add end-to-end UX tests (Playwright) to verify flows and accessibility",
            "details": [
              "Install Playwright: npm init playwright@latest (or npm i -D @playwright/test)",
              "Create test: tests/e2e/ux.spec.ts",
              "Test user flow: open app, navigate to token input, type valid token, submit, open Help modal, verify accessibility attributes (role dialog, focus)",
              "Add accessibility check using @playwright/test's accessibility snapshot or axe-playwright integration",
              "Add script to package.json: \"test:e2e\": \"playwright test\"",
              "Run: npx playwright test tests/e2e/ux.spec.ts",
              "Acceptance: test passes across default browser(s)",
              "Commit e2e tests:",
              "Run: git add tests/e2e/ux.spec.ts playwright.config.* && git commit -m \"test: add Playwright e2e tests verifying UX flows and accessibility\""
            ],
            "done": false
          },
          {
            "number": "9.13",
            "description": "Replace important hard-coded strings with i18n keys and add English resource",
            "details": [
              "Create directory: src/i18n/",
              "Create file: src/i18n/en.json containing keys used in UI (e.g., \"help.title\": \"Help\", \"token.placeholder\": \"Enter token\", \"token.error.invalid\": \"Token is invalid\")",
              "Update TokenInput.jsx and HelpModal.jsx to import strings from src/i18n/en.json and reference them (simple key lookup)",
              "Acceptance: files updated and no hard-coded strings remain for label, placeholder, and error in those components",
              "Create a small test: tests/unit/test_i18n_keys.test.js to assert that each used key exists in en.json",
              "Commit i18n changes:",
              "Run: git add src/i18n/en.json src/components/TokenInput.jsx src/components/HelpModal.jsx tests/unit/test_i18n_keys.test.js && git commit -m \"feat: add i18n resource file and replace hard-coded UI strings\""
            ],
            "done": false
          },
          {
            "number": "9.14",
            "description": "Add plain-language UX examples and error message guidelines",
            "details": [
              "Update docs/STYLE_GUIDE.md with a subsection \"Error & Help Copy\" listing:",
              "Short actionable messages, examples for token errors, and recovery steps",
              "Example error format: \"What happened\" + \"What you can do\" + \"If you need help\" (one-liners)",
              "Update docs/USAGE.md to include a \"Common errors\" table with sample user-visible messages and recommended fixes",
              "Acceptance: both files contain at least 3 examples of error messages and one \"Recovery\" step per example",
              "Commit copy guidelines:",
              "Run: git add docs/STYLE_GUIDE.md docs/USAGE.md && git commit -m \"docs: add plain-language examples and error message guidelines\""
            ],
            "done": false
          },
          {
            "number": "9.15",
            "description": "Add visible examples and screenshots for the docs site",
            "details": [
              "Create directory: docs/screenshots/",
              "Capture screenshots (manual or using Playwright) at three viewports: desktop (1280x800), tablet (768x1024), mobile (375x667)",
              "Save files: docs/screenshots/desktop.png, docs/screenshots/tablet.png, docs/screenshots/mobile.png",
              "If using Playwright: npx playwright show-trace or create a script tests/e2e/screenshots.spec.ts that saves page.screenshot()",
              "Reference images in docs/UX.md and docs/USAGE.md with alt text (e.g., \"Main app view on desktop\")",
              "Acceptance: images exist and docs reference them (grep \"screenshots/\" docs/)",
              "Commit screenshots and docs updates:",
              "Run: git add docs/screenshots/* docs/UX.md docs/USAGE.md && git commit -m \"docs: add screenshots for responsive examples\""
            ],
            "done": false
          },
          {
            "number": "9.16",
            "description": "Configure linters and formatters and run them",
            "details": [
              "Add ESLint and Prettier configs:",
              "Create .eslintrc.json with recommended rules for React (or base) and .prettierrc with preferred formatting settings",
              "Install dev dependencies: npm install --save-dev eslint prettier eslint-config-prettier eslint-plugin-react",
              "Run format: npx prettier --write \"src/**/*.{js,jsx,ts,tsx,json,css,md}\"",
              "Run linter and autofix: npx eslint \"src/**/*.{js,jsx,ts,tsx}\" --fix",
              "Acceptance: npx eslint \"src/**/*.{js,jsx,ts,tsx}\" exits with code 0 (no errors), and git status shows only intended changes",
              "Commit formatting/lint config and fixes:",
              "Run: git add .eslintrc.json .prettierrc package.json package-lock.json && git commit -m \"chore: add ESLint/Prettier configs and apply fixes\""
            ],
            "done": false
          },
          {
            "number": "9.17",
            "description": "Add documentation for running tests and linting to docs",
            "details": [
              "Update docs/ACCESSIBILITY.md and docs/README.md with explicit commands and expected outcomes:",
              "Include: npm test (unit), npm run test:e2e (e2e), npm run audit:pa11y, node scripts/check-contrast.js, npx prettier --check src/",
              "Add a \"CI checklist\" section listing all required checks for PRs",
              "Acceptance: docs files contain a \"Running Tests\" section with the listed commands",
              "Commit test docs:",
              "Run: git add docs/ACCESSIBILITY.md docs/README.md && git commit -m \"docs: document test, lint, and accessibility commands for contributors\""
            ],
            "done": false
          },
          {
            "number": "9.18",
            "description": "Add CHANGELOG entry and release note for UX & accessibility work",
            "details": [
              "Create or update CHANGELOG.md with a section \"Unreleased\" and bullet points:",
              "\"Added accessible Help modal, TokenInput improvements, focus management, automated accessibility tests, style guide\"",
              "Add links in the changelog to relevant docs and tests",
              "Acceptance: CHANGELOG.md contains an \"Unreleased\" heading and 4+ bullets describing the work",
              "Commit changelog:",
              "Run: git add CHANGELOG.md && git commit -m \"docs: update changelog with UX & accessibility updates\""
            ],
            "done": false
          },
          {
            "number": "9.19",
            "description": "Run full quality gate and fix any remaining issues",
            "details": [
              "Run formatters and linters:",
              "npx prettier --write \"src/**/*.{js,jsx,ts,tsx,json,css,md}\"",
              "npx eslint \"src/**/*.{js,jsx,ts,tsx}\" --max-warnings=0",
              "Run all tests:",
              "npx jest --coverage",
              "npx playwright test",
              "npx jest tests/accessibility/test_a11y.test.jsx",
              "Run accessibility audits:",
              "npm run audit:pa11y (ensure dev server running)",
              "./scripts/run-lighthouse.sh",
              "Acceptance: All tests pass, ESLint exits 0, pa11y produces no critical errors, lighthouse report is generated",
              "If fixes are required, make edits and commit iteratively:",
              "Example commit after fixes: git add [changed files] && git commit -m \"fix: address accessibility/lint/test failures\""
            ],
            "done": false
          },
          {
            "number": "9.20",
            "description": "Final documentation commit and release-ready commit",
            "details": [
              "Aggregate final docs and code changes to ensure all work is tracked:",
              "Run: git add docs/ src/components/ src/utils/ tests/ package.json package-lock.json CHANGELOG.md",
              "Final commit:",
              "Run: git commit -m \"docs: finalize UX & accessibility docs; feat: finalize accessible components and tests\"",
              "Push branch (if applicable):",
              "Run: git push origin HEAD",
              "Acceptance: All relevant files are committed, tests/linting pass on local machine, docs present in docs/ and linked from README.md"
            ],
            "done": false
          }
        ]
      },
      {
        "number": 10,
        "title": "Release, Distribution & Post\u2011MVP Enhancements",
        "description": null,
        "steps": [
          {
            "number": "10.1",
            "description": "Create a persistent version file `VERSION`",
            "details": [
              "Create file `VERSION` at repo root containing the current semantic version, e.g. `0.1.0`",
              "Add a one-line shell script `scripts/bump_version.sh` with usage: `./scripts/bump_version.sh patch|minor|major` that:",
              "Reads `VERSION`, splits into MAJOR.MINOR.PATCH",
              "Increments the requested part, writes back new version to `VERSION`",
              "Prints new version to stdout",
              "Creates a new git commit with message `chore: bump version to x.y.z` and a git tag `vX.Y.Z` (use `git commit -m` and `git tag -a`)",
              "Make `scripts/bump_version.sh` executable: `chmod +x scripts/bump_version.sh`",
              "Acceptance checks:",
              "Running `./scripts/bump_version.sh patch` updates `VERSION` and prints new version",
              "A git commit and tag exist after running the script (verify `git log -1` and `git tag --list | tail -n1`)",
              "Commit:",
              "Run: git add VERSION scripts/bump_version.sh && git commit -m \"chore: add VERSION file and bump_version script\""
            ],
            "done": false
          },
          {
            "number": "10.2",
            "description": "Add changelog management: create `CHANGELOG.md` and a changelog generator script",
            "details": [
              "Create `CHANGELOG.md` at repo root with a template header:",
              "Add sections: \"Unreleased\" and an example entry format using conventional commits",
              "Example content: `# Changelog\\n\\n## Unreleased\\n\\n- (use conventional commits to list changes)\\n`",
              "Create `scripts/generate_changelog.sh`:",
              "Uses `git log --pretty=format:\"%s\"` to collect commits since last tag",
              "Formats listed commits under the newest version (reads `VERSION`) and appends to top of `CHANGELOG.md` replacing \"Unreleased\" with `## vX.Y.Z - YYYY-MM-DD`",
              "Leaves the \"Unreleased\" section empty for future changes",
              "Make script executable",
              "Acceptance checks:",
              "Running `./scripts/generate_changelog.sh` inserts a dated release entry into `CHANGELOG.md` using the version from `VERSION` (create a test commit first)",
              "Commit:",
              "Run: git add CHANGELOG.md scripts/generate_changelog.sh && git commit -m \"chore: add CHANGELOG template and generator\""
            ],
            "done": false
          },
          {
            "number": "10.3",
            "description": "Create a reproducible release artifact script `scripts/release.sh`",
            "details": [
              "Create `scripts/release.sh` (executable) that:",
              "Reads `VERSION` and creates a compressed archive `dist/token-muncher-vX.Y.Z.tar.gz` containing the repo files required for runtime (exclude dev directories using `tar --exclude` for `.git`, `tests`, `node_modules`, `.venv` etc.)",
              "Creates a `dist/token-muncher-vX.Y.Z.zip` in Windows-compatible format as well",
              "Verifies both artifacts exist after creation and prints their paths",
              "Optionally calls `scripts/generate_changelog.sh` before packaging",
              "Exits non-zero if any step fails",
              "Add acceptance checks in the script (verify `tar -tzf` and `unzip -l`) and print \"OK\" when done",
              "Make executable: `chmod +x scripts/release.sh`",
              "Commit:",
              "Run: git add scripts/release.sh && git commit -m \"chore: add release packaging script\""
            ],
            "done": false
          },
          {
            "number": "10.4",
            "description": "Add a container image definition `Dockerfile` and entry script",
            "details": [
              "Create `Dockerfile` at repo root with minimal, generic structure:",
              "Use `FROM alpine:3.18` (small base) or `python:3.11-alpine` if repo contains Python files (if Python not present, the Alpine base is fine)",
              "Create `/app` directory, copy necessary runtime files into `/app`",
              "Add user `appuser`, set `USER appuser`",
              "Add `ENTRYPOINT [\"/app/start.sh\"]`",
              "Create `scripts/start.sh` at `scripts/start.sh` and make executable:",
              "Detect runtime: if `./token-muncher` binary exists, run it",
              "Else if `package.json` exists run `node index.js`",
              "Else if `setup.py` or `pyproject.toml` exists run `python -m token_muncher` (safe fallback)",
              "Export `PORT` default `8080` and log start: `echo \"Starting token muncher on port $PORT\"`",
              "Exit with non-zero if no runtime is found",
              "Acceptance checks:",
              "`docker build -t token-muncher:local .` completes successfully",
              "`docker run --rm token-muncher:local` executes `start.sh` and exits with status 0 or logs expected error when runtime missing",
              "Commit:",
              "Run: git add Dockerfile scripts/start.sh && git commit -m \"feat: add Dockerfile and start script\""
            ],
            "done": false
          },
          {
            "number": "10.5",
            "description": "Add a simple healthcheck and monitoring scripts under `scripts/`",
            "details": [
              "Create `scripts/healthcheck.sh`:",
              "Accept optional `PORT` arg or env var with default `8080`",
              "Try `curl --fail http://localhost:$PORT/health` with timeout 3s; if no HTTP server exists, check for a `token-muncher` process via `pgrep -f token-muncher`",
              "Exit 0 if either check succeeds, else exit 2",
              "Create `scripts/monitor_example.sh` demonstrating a simple loop:",
              "Runs `healthcheck.sh` every 30s and logs results to `logs/health.log`",
              "Creates `logs/` directory if missing",
              "Acceptance checks:",
              "`./scripts/healthcheck.sh` returns 2 when nothing is running and 0 if a dummy server is started",
              "`./scripts/monitor_example.sh` creates `logs/health.log`",
              "Commit:",
              "Run: git add scripts/healthcheck.sh scripts/monitor_example.sh && git commit -m \"feat: add healthcheck and monitor example scripts\""
            ],
            "done": false
          },
          {
            "number": "10.6",
            "description": "Create a generic CI release workflow `.github/workflows/release.yml`",
            "details": [
              "Create directory `.github/workflows/` and file `release.yml` with a workflow triggered on `push` of tags `v*.*.*`",
              "Steps in workflow:",
              "Checkout code",
              "Run `scripts/bump_version.sh` only if needed (skip if tag exists)",
              "Run `scripts/generate_changelog.sh`",
              "Build artifacts: run `./scripts/release.sh`",
              "Build Docker image and push to registry (placeholder using `docker/build-push-action`) with image name `${{ github.repository }}:v${{ steps.get_version.outputs.version }}`",
              "Create a GitHub Release and upload `dist/*.tar.gz` and `dist/*.zip`",
              "Add notes in the YAML to use secrets `DOCKER_REGISTRY`, `DOCKER_USERNAME`, `DOCKER_PASSWORD`, `GITHUB_TOKEN`",
              "Acceptance checks:",
              "A push of tag `v0.1.0` will start the workflow (can be tested with a dry run on a fork)",
              "Commit:",
              "Run: git add .github/workflows/release.yml && git commit -m \"chore: add CI release workflow\""
            ],
            "done": false
          },
          {
            "number": "10.7",
            "description": "Add generic tests for release artifacts and scripts under `tests/release/`",
            "details": [
              "Create `tests/release/test_release_artifacts.sh` (executable) that:",
              "Invokes `./scripts/release.sh`",
              "Verifies `dist/token-muncher-v$(cat VERSION).tar.gz` exists and its contents include `start.sh` and `VERSION` (use `tar -tzf` and `grep`)",
              "Verifies `dist/token-muncher-vX.Y.Z.zip` exists and lists same files",
              "Fail with non-zero exit code on mismatch",
              "Create `tests/release/test_healthcheck.sh` that:",
              "Starts a simple HTTP server (e.g., `python -m http.server 8080` in background if Python present) with `/health` route fallback via `while true; do echo -e \"HTTP/1.1 200 OK\\r\\n\\r\\nOK\" | nc -l -p 8080; done` (portable variant)",
              "Runs `scripts/healthcheck.sh` and expects exit code 0",
              "Kill background server at end",
              "Run tests by shell: `bash tests/release/test_release_artifacts.sh` `bash tests/release/test_healthcheck.sh`",
              "Commit:",
              "Run: git add tests/release/test_release_artifacts.sh tests/release/test_healthcheck.sh && git commit -m \"test: add release and healthcheck integration tests\""
            ],
            "done": false
          },
          {
            "number": "10.8",
            "description": "Add repository-wide quality hooks and run formatters/linters",
            "details": [
              "Create `.pre-commit-config.yaml` at repo root with basic hooks:",
              "`trailing-whitespace`, `end-of-file-fixer`, `check-yaml`, and `check-added-large-files`",
              "If repo contains Python files (detect `.py`), add `black` and `flake8` hooks (document how to enable)",
              "Add a small script `scripts/run_linters.sh`:",
              "Installs pre-commit if not installed: `pip install pre-commit` (if python not available, echo a message)",
              "Runs `pre-commit run --all-files`",
              "If any `.py` files exist, run `black --check .` and `flake8 .` (skip with notice if tools not installed)",
              "Exit with non-zero on failures",
              "Acceptance checks:",
              "`scripts/run_linters.sh` exits 0 when codebase meets hook rules",
              "Running `pre-commit run --all-files` succeeds locally after installing hooks",
              "Commit:",
              "Run: git add .pre-commit-config.yaml scripts/run_linters.sh && git commit -m \"chore: add pre-commit config and linter runner\""
            ],
            "done": false
          },
          {
            "number": "10.9",
            "description": "Add documentation updates and end-user release notes under `docs/`",
            "details": [
              "Create `docs/RELEASE_NOTES.md` with:",
              "Template that instructs maintainers how to write release notes (link to `CHANGELOG.md` and what to include)",
              "Example \"How to install the release\" with commands to download and extract `dist/token-muncher-vX.Y.Z.tar.gz` and run `./start.sh`",
              "Create `docs/USAGE.md`:",
              "Add quickstart, example commands to run the app, environment variables used (PORT, TELEMETRY_ENABLED)",
              "Add CLI help example: `./token-muncher --help` or `node index.js --help` and a small sample output",
              "Create `docs/PRIVACY.md` explaining telemetry opt-in behavior (see step 10.12)",
              "Acceptance checks:",
              "`docs/USAGE.md` contains at least one working example command",
              "`docs/RELEASE_NOTES.md` contains an install and verify section",
              "Commit:",
              "Run: git add docs/RELEASE_NOTES.md docs/USAGE.md docs/PRIVACY.md && git commit -m \"docs: add release notes, usage, and privacy docs\""
            ],
            "done": false
          },
          {
            "number": "10.10",
            "description": "Add a simple telemetry opt-in implementation and privacy notice",
            "details": [
              "Create `scripts/telemetry.sh`:",
              "Checks env var `TELEMETRY_ENABLED` (default `false`)",
              "If `true`, logs a line \"Telemetry: sending anon event VERSION=...\" to `logs/telemetry.log`",
              "If `false`, prints \"Telemetry disabled\"",
              "Ensure this script is safe to run in PoC mode (does NOT contact external servers by default)",
              "Update `docs/PRIVACY.md`:",
              "Document how telemetry works and how to opt-in/out",
              "Add tests `tests/release/test_telemetry.sh`:",
              "Set `TELEMETRY_ENABLED=true` run `./scripts/telemetry.sh` and assert `logs/telemetry.log` contains version",
              "Set `TELEMETRY_ENABLED=false` and assert output contains \"Telemetry disabled\"",
              "Acceptance checks:",
              "Running the telemetry test script creates `logs/telemetry.log` when enabled and no network calls are made",
              "Commit:",
              "Run: git add scripts/telemetry.sh docs/PRIVACY.md tests/release/test_telemetry.sh && git commit -m \"feat: add telemetry opt-in script and privacy docs\""
            ],
            "done": false
          },
          {
            "number": "10.11",
            "description": "Add simple feature flags config and loader for post\u2011MVP toggles",
            "details": [
              "Create `config/feature_flags.json`:",
              "Example content: {\"new_ui\": false, \"fast_parser\": true}",
              "Document expected keys and types in the file header comment",
              "Create `scripts/load_feature_flag.sh`:",
              "Usage: `./scripts/load_feature_flag.sh new_ui`",
              "Reads `config/feature_flags.json` and uses `jq` if available or simple `grep` fallback to print `true` or `false`",
              "Exit code 0 if feature exists; 3 if feature key not found",
              "Acceptance checks:",
              "`./scripts/load_feature_flag.sh new_ui` prints `false` with exit code 0",
              "`./scripts/load_feature_flag.sh unknown` exits 3 and prints a helpful error message",
              "Commit:",
              "Run: git add config/feature_flags.json scripts/load_feature_flag.sh && git commit -m \"feat: add feature flags config and loader\""
            ],
            "done": false
          },
          {
            "number": "10.12",
            "description": "Provide user-facing help text and examples in `README.md` and sample CLI help",
            "details": [
              "Update `README.md` at repo root:",
              "Add short description \"token muncher\", current version shown from `VERSION` by example",
              "Add \"Installation\" with commands to download tarball from `dist/` and extract",
              "Add \"Usage\" section showing `./start.sh`, `./scripts/healthcheck.sh`, `./scripts/load_feature_flag.sh new_ui`",
              "Add \"Error messages\" section documenting common error strings (e.g., \"Runtime not found: no token-muncher binary/node/python entrypoint\" and how to fix)",
              "Add sample CLI help script `scripts/print_help.sh` that prints help text and examples for end users (also used in README)",
              "Acceptance checks:",
              "`bash scripts/print_help.sh` prints the same usage text as README's usage section",
              "README contains the `VERSION` example and an \"Error messages\" section",
              "Commit:",
              "Run: git add README.md scripts/print_help.sh && git commit -m \"docs: update README with install/usage/help and error message guidance\""
            ],
            "done": false
          },
          {
            "number": "10.13",
            "description": "Run full quality pipeline locally and produce a release candidate tag",
            "details": [
              "Execute sequentially:",
              "`./scripts/run_linters.sh` \u2014 fix any issues reported",
              "`bash tests/release/test_release_artifacts.sh` and other tests in `tests/release/` \u2014 fix failing tests",
              "`black` or `prettier` if using specific language (document fallback in `scripts/run_linters.sh`)",
              "If all checks pass, bump a patch version and create a release tag:",
              "Run: `./scripts/bump_version.sh patch`",
              "Confirm `VERSION` changed and a tag `vX.Y.Z` was created (`git show vX.Y.Z`)",
              "Commit step is inside bump script, but also record release helper commit if needed:",
              "If any manual file changes were made after tests, run: git add [files] && git commit -m \"chore: final release prep\"",
              "Acceptance checks:",
              "All lint/test scripts exit 0",
              "A git tag exists for the created version",
              "Commit:",
              "If not already committed by bump script: git add VERSION CHANGELOG.md && git commit -m \"chore: final release prep\""
            ],
            "done": false
          },
          {
            "number": "10.14",
            "description": "Publish release artifacts and create GitHub Release (manual or automated)",
            "details": [
              "Manual steps (if automating via CI, skip manual push):",
              "Push tags and commits: `git push origin main --follow-tags` (or the default branch)",
              "Run `./scripts/release.sh` locally to create `dist/` artifacts",
              "Upload `dist/token-muncher-vX.Y.Z.tar.gz` and `dist/token-muncher-vX.Y.Z.zip` to a GitHub Release named `vX.Y.Z` and paste contents of `CHANGELOG.md` section for the version into the release notes",
              "Optionally push Docker image:",
              "`docker build -t myorg/token-muncher:vX.Y.Z .`",
              "`docker push myorg/token-muncher:vX.Y.Z` (requires Docker credentials)",
              "Automated path (CI):",
              "Ensure `.github/workflows/release.yml` is configured with proper secrets and that pushing the tag in step 10.13 triggered the workflow",
              "Verify the workflow completed successfully and released artifacts appear under the GitHub Release",
              "Acceptance checks:",
              "`dist/` contains tar.gz and zip artifacts",
              "GitHub Release exists with uploaded artifacts and changelog text",
              "Docker image tagged `vX.Y.Z` is visible in the registry (if pushed)",
              "Commit:",
              "After any manual metadata edits: git add CHANGELOG.md docs/RELEASE_NOTES.md && git commit -m \"docs: publish release notes for v$(cat VERSION)\""
            ],
            "done": false
          },
          {
            "number": "10.15",
            "description": "Plan and scaffold post\u2011MVP enhancement tickets and skeletons",
            "details": [
              "Create `docs/ROADMAP.md` listing prioritized post-MVP items (examples):",
              "\"1. Improve parser performance\", \"2. UI refresh (flagged behind new_ui)\", \"3. Persistence / DB integration\", \"4. Multi-tenant telemetry\"",
              "For each item add a short acceptance criteria bullet list",
              "Create `issues/` directory and a file per feature placeholder `issues/001-improve-parser.md`, `issues/002-ui-refresh.md`:",
              "Each file contains: description, motivation, proposed APIs/config, tests to add, estimated effort",
              "Include link to `config/feature_flags.json` for feature flag wiring",
              "Add basic skeleton PR template `.github/pull_request_template.md` instructing contributors to reference `CHANGELOG.md` and `VERSION`",
              "Acceptance checks:",
              "`docs/ROADMAP.md` lists at least 4 next-phase features with acceptance criteria",
              "`issues/` contains at least 2 issue skeleton files",
              "Commit:",
              "Run: git add docs/ROADMAP.md issues/001-improve-parser.md issues/002-ui-refresh.md .github/pull_request_template.md && git commit -m \"docs: add roadmap and post-MVP issue skeletons\""
            ],
            "done": false
          },
          {
            "number": "10.16",
            "description": "Final verification, cleanup, and developer onboarding notes",
            "details": [
              "Create `DEVELOPER_ONBOARDING.md` describing:",
              "How to run the app locally (`./start.sh`), run tests (`bash tests/release/*.sh`), run linters (`./scripts/run_linters.sh`)",
              "How to create a release (bump -> generate changelog -> run release -> push tag)",
              "Where to find Docker images and how to push them",
              "Run a final smoke sequence and record commands in a new file `scripts/smoke_test.sh` to reproduce:",
              "`./scripts/run_linters.sh`",
              "`bash tests/release/test_release_artifacts.sh`",
              "`bash tests/release/test_healthcheck.sh`",
              "`bash tests/release/test_telemetry.sh`",
              "Acceptance checks:",
              "`scripts/smoke_test.sh` exits 0 on a clean environment",
              "`DEVELOPER_ONBOARDING.md` contains the exact CLI commands used in `scripts/smoke_test.sh`",
              "Commit:",
              "Run: git add DEVELOPER_ONBOARDING.md scripts/smoke_test.sh && git commit -m \"docs: add developer onboarding and smoke test script\""
            ],
            "done": false
          }
        ]
      }
    ],
    "summary": "Development plan for token muncher with 10 phases",
    "raw_basic_response": "1. **Phase 1: Project Initialization & Tooling**  \n   Establish the repository, development environment, and core project scaffolding to ensure reproducible builds and consistent developer workflows.  \n   - Initialize Git repository, LICENSE, README, and basic project layout (src/, tests/, docs/)  \n   - Choose packaging and env tooling (poetry or pip+venv) and create lock file / requirements  \n   - Add linters/formatters and hooks (black, ruff/flake8, mypy, pre-commit)  \n   - Create CI skeleton (GitHub Actions) with test/lint matrix and basic Windows/Linux runners  \n   - Provide dev container / Dockerfile and platform install notes (tcod vs rich alternatives)\n\n2. **Phase 2: Core Game Model & Engine**  \n   Build the testable, rendering-agnostic game core (model, engine, EventBus) and deterministic turn loop so logic can be exercised without UI.  \n   - Define dataclasses and type-hinted domain model (map, actors, tokens, items, game state)  \n   - Implement Engine, command processing, and deterministic turn loop with EventBus abstraction  \n   - Create interfaces for Renderer and InputController (headless implementations for tests)  \n   - Add unit tests for core rules (movement, collisions, token pickup) and deterministic replay via seeds  \n   - Establish logging and simple instrumentation hooks\n\n3. **Phase 3: Map Generation & Token Placement**  \n   Implement procedural map generation and token placement systems with reproducible seeds and evaluation metrics.  \n   - Implement map generator(s) (recursive backtracker / Prim / cellular automata) and map API  \n   - Add token placement algorithms and spawn rules with density and fairness constraints  \n   - Implement map evaluation (connectivity, path length, token accessibility) and candidate selection  \n   - Provide seed-based reproducibility and utilities for debugging/generation visualization  \n   - Unit tests for generation properties and connectivity\n\n4. **Phase 4: Rendering & Input Layer**  \n   Create platform-specific rendering and input modules while keeping core logic decoupled; supply an alternative minimal renderer for environments where tcod is unavailable.  \n   - Implement tcod-based renderer and input adapter (or rich+readchar alternative) following Renderer/InputController interfaces  \n   - Implement HUD, keybindings, help overlay, and keyboard mapping with config-driven bindings  \n   - Optimize drawing (dirty tiles/partial redraw) and provide fallback ASCII/text renderer for CI/Windows tests  \n   - Add cross-platform input handling and Windows-specific notes (windows-curses guidance if needed)  \n   - Integration tests for render loop and input handling (headless mocks + smoke tests)\n\n5. **Phase 5: Player Mechanics, Items & Tokens**  \n   Add player actions, inventory/pickups, token collection mechanics, and basic game rules tying model and renderer together.  \n   - Implement action dispatchers (move, wait, pick up, use item) and validation with undoable/atomic state changes  \n   - Add item/token data models and pickup/resolution systems (score, effects, temporary buffs)  \n   - Implement HUD updates, score display, and in-game feedback for actions and errors  \n   - Configurable game tables for tokens/items (JSON) to allow tuning without code changes  \n   - Tests for action resolution, inventories, and token collection edge cases\n\n6. **Phase 6: Enemy AI & Pathfinding**  \n   Implement enemy behaviors and pathfinding so adversaries interact meaningfully with the player and environment.  \n   - Implement simple A* pathfinder or integrate tcod.path if chosen; provide unit tests for pathfinder correctness  \n   - Implement enemy FSM behaviors (patrol, chase, scatter, frightened) and behavior parameters in config  \n   - Integrate AI decision loop into Engine turn processing with randomness hooks for variability  \n   - Add spawn rules, difficulty scaling, and balancing parameters (tweakable via JSON)  \n   - Tests/simulations for common combat scenarios and avoidance of obvious traps/unfair behavior\n\n7. **Phase 7: Persistence, Saves & Configuration Management**  \n   Add save/load for sessions and meta-progression, plus robust configuration and migration strategies.  \n   - Implement save/load system with schema versioning and atomic writes (write-temp + rename) using JSON and optional sqlite metadata  \n   - Provide migration hooks and backward-compatibility strategies for saved data  \n   - Implement player profiles, session resumes, and options persistence (keybindings, graphics choice)  \n   - Implement export/import, seed sharing, and replay support for deterministic runs  \n   - Tests for save/load roundtrips and migration scenarios\n\n8. **Phase 8: Testing, QA & CI Maturity**  \n   Harden the codebase with comprehensive automated tests, static checks, and reproducible CI pipelines.  \n   - Expand unit and integration tests (core systems, AI, generation, save/load, renderer headless tests) using pytest  \n   - Add coverage reporting and performance/regression smoke tests for critical loops  \n   - Enforce mypy and linter rules in CI; tune rules to avoid developer friction  \n   - Create test matrices across OSes and Python versions; include reproducible seed-based test cases  \n   - Add fuzz or property-based tests for generator/AI invariants where helpful\n\n9. **Phase 9: UX, Accessibility & Documentation**  \n   Polish the user experience and provide documentation so users and contributors can run and extend the project.  \n   - Create in-game help, tutorial/first-run experience, and contextual tooltips for controls and objectives  \n   - Implement robust input validation, clear error messages, and accessible color schemes (contrast, color-blind modes)  \n   - Prepare user documentation: README, quickstart, configuration guide, and troubleshooting (installation notes for tcod/Windows)  \n   - Produce developer docs: architecture overview, contribution guide, code style, and testing guide  \n   - Add sample configs, recorded replays, and example seeds\n\n10. **Phase 10: Release, Distribution & Post\u2011MVP Enhancements**  \n   Prepare the project for public distribution, gather early user feedback, and plan iterative improvements based on analytics and playtesting.  \n   - Package and publish artifacts (wheel via poetry, GitHub Releases or PyPI) and provide platform-specific install instructions  \n   - Finalize release notes, changelog, and marketing README/screenshots/tutorial assets  \n   - Optional opt-in analytics/logging (local only), telemetry hooks, and instrumentation for balancing decisions  \n   - Plan and prioritize post\u2011MVP features: achievements, new enemy types, level progression, and performance optimizations  \n   - Set up issue templates, backlog for community contributions, and roadmap for subsequent iterations",
    "raw_detailed_responses": null
  },
  "stage": "handoff",
  "name": "66",
  "timestamp": 1763609588,
  "id": "1763609588_66"
}